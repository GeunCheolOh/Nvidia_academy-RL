#!/usr/bin/env python3
"""
FrozenLake Q-Learning í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import numpy as np
import gymnasium as gym
import pygame
import time
import sys
import os
from tqdm import tqdm

# ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€ (utils ì„í¬íŠ¸ìš©)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.io import load_q_table, load_hyperparameters


class QLearningEvaluator:
    def __init__(self, q_table, render_mode="human"):
        """
        Q-Learning í‰ê°€ì ì´ˆê¸°í™”
        
        Args:
            q_table: í›ˆë ¨ëœ Q-table
            render_mode: ë Œë”ë§ ëª¨ë“œ ("human", "rgb_array", ë˜ëŠ” None)
        """
        self.q_table = q_table
        self.render_mode = render_mode
        
    def choose_action(self, state):
        """
        greedy ì •ì±…ì„ ì‚¬ìš©í•˜ì—¬ í–‰ë™ ì„ íƒ (íƒí—˜ ì—†ìŒ)
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
            
        Returns:
            Q-tableì— ë”°ë¥¸ ìµœì„ ì˜ í–‰ë™
        """
        # TODO: Q-tableì„ ì‚¬ìš©í•˜ì—¬ ìµœì„ ì˜ í–‰ë™ì„ ì„ íƒí•˜ì„¸ìš” (greedy ì •ì±…)
        # íŒíŠ¸ 1: self.q_table[state]ë¡œ í˜„ì¬ ìƒíƒœì˜ ëª¨ë“  Q-ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤
        # íŒíŠ¸ 2: np.max()ë¡œ ìµœëŒ€ Q-ê°’ì„ ì°¾ìŠµë‹ˆë‹¤
        # íŒíŠ¸ 3: np.where()ë¡œ ìµœëŒ€ Q-ê°’ì„ ê°€ì§„ í–‰ë™ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤
        # íŒíŠ¸ 4: ë™ì ì¸ ê²½ìš° np.random.choice()ë¡œ ë¬´ì‘ìœ„ ì„ íƒí•©ë‹ˆë‹¤
        #YOUR CODE HERE
        raise NotImplementedError("greedy í–‰ë™ ì„ íƒì„ êµ¬í˜„í•˜ì„¸ìš”")
    
    def evaluate_episodes(self, env, num_episodes, max_steps_per_episode=100, verbose=True):
        """
        ì—¬ëŸ¬ ì—í”¼ì†Œë“œì— ëŒ€í•´ ì—ì´ì „íŠ¸ë¥¼ í‰ê°€
        
        Args:
            env: Gymnasium í™˜ê²½
            num_episodes: í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜
            max_steps_per_episode: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜
            verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€
            
        Returns:
            í‰ê°€ í†µê³„ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        episode_rewards = []
        episode_lengths = []
        success_episodes = []
        
        if verbose:
            print(f"{num_episodes} ì—í”¼ì†Œë“œì— ëŒ€í•´ ì—ì´ì „íŠ¸ í‰ê°€ ì¤‘...")
            print(f"í™˜ê²½: {env.spec.id}")
            print()
        
        # í‰ê°€ ë£¨í”„
        for episode in tqdm(range(num_episodes), desc="í‰ê°€ ì¤‘", disable=not verbose):
            # TODO: í™˜ê²½ì„ ë¦¬ì…‹í•˜ê³  ì´ˆê¸° ìƒíƒœë¥¼ ë°›ìœ¼ì„¸ìš”
            # íŒíŠ¸: env.reset()ì€ (state, info) íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤
            #YOUR CODE HERE
            raise NotImplementedError("í™˜ê²½ ë¦¬ì…‹ì„ êµ¬í˜„í•˜ì„¸ìš”")
            
            total_reward = 0
            steps = 0
            
            for step in range(max_steps_per_episode):
                # ìµœì„ ì˜ í–‰ë™ ì„ íƒ (greedy ì •ì±…)
                action = self.choose_action(state)
                
                # TODO: í–‰ë™ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°›ìœ¼ì„¸ìš”
                # íŒíŠ¸ 1: env.step(action)ì„ í˜¸ì¶œí•©ë‹ˆë‹¤
                # íŒíŠ¸ 2: ë°˜í™˜ê°’: (next_state, reward, terminated, truncated, info)
                # íŒíŠ¸ 3: done = terminated or truncatedë¡œ ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€ í™•ì¸
                #YOUR CODE HERE
                raise NotImplementedError("í™˜ê²½ stepì„ êµ¬í˜„í•˜ì„¸ìš”")
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            # ì—í”¼ì†Œë“œ í†µê³„ ê¸°ë¡
            episode_rewards.append(total_reward)
            episode_lengths.append(steps)
            success_episodes.append(1 if total_reward > 0 else 0)
        
        # ìµœì¢… í†µê³„ ê³„ì‚°
        avg_reward = np.mean(episode_rewards)
        success_rate = np.mean(success_episodes) * 100
        avg_length = np.mean(episode_lengths)
        
        if verbose:
            print(f"\nEvaluation Results:")
            print(f"  ì—í”¼ì†Œë“œ: {num_episodes}")
            print(f"  í‰ê·  ë³´ìƒ: {avg_reward:.3f}")
            print(f"  ì„±ê³µë¥ : {success_rate:.1f}%")
            print(f"  í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´: {avg_length:.1f} ìŠ¤í…")
        
        return {
            'num_episodes': num_episodes,
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'success_episodes': success_episodes,
            'average_reward': avg_reward,
            'success_rate': success_rate,
            'average_length': avg_length
        }
    
    def demonstrate_policy(self, env, num_episodes=3, step_delay=0.8, verbose=True, interactive=True):
        """
        í•™ìŠµëœ ì •ì±…ì„ ì‹œê°ì  ë Œë”ë§ìœ¼ë¡œ ì‹œì—°
        
        Args:
            env: Gymnasium í™˜ê²½
            num_episodes: ì‹œì—°í•  ì—í”¼ì†Œë“œ ìˆ˜
            step_delay: ìŠ¤í… ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
            verbose: ìŠ¤í… ì •ë³´ ì¶œë ¥ ì—¬ë¶€
            interactive: ì—í”¼ì†Œë“œ ê°„ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì—¬ë¶€
        """
        if self.render_mode not in ["human", "rgb_array"]:
            print("ê²½ê³ : ì‹œì—°ì€ render_mode='human' ë˜ëŠ” 'rgb_array'ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return
        
        print(f"ğŸ® ì—ì´ì „íŠ¸ í”Œë ˆì´ ì‹œì—° ì‹œì‘!")
        print(f"ğŸ“º ì—í”¼ì†Œë“œ ìˆ˜: {num_episodes}")
        print(f"â±ï¸  ìŠ¤í… ë”œë ˆì´: {step_delay}ì´ˆ")
        print(f"ğŸ¯ ëª©í‘œ: S(ì‹œì‘) â†’ G(ê³¨) ë„ë‹¬, H(êµ¬ë©) í”¼í•˜ê¸°")
        print()
        
        successes = 0
        
        for episode in range(num_episodes):
            print(f"ğŸ¬ Episode {episode + 1}/{num_episodes}")
            print("=" * 40)
            
            state, _ = env.reset()
            total_reward = 0
            steps = 0
            path = [state]  # ê²½ë¡œ ì¶”ì 
            
            if verbose:
                print(f"ğŸ ì‹œì‘ ìœ„ì¹˜: {state} (ìœ„ì¹˜: {state//4}, {state%4})")
            
            # ì´ˆê¸° ìƒíƒœ ë Œë”ë§
            if self.render_mode == "human":
                env.render()
                time.sleep(step_delay)
            
            max_steps = 100
            while steps < max_steps:
                # í˜„ì¬ ìƒíƒœì˜ Q-ê°’ë“¤ í‘œì‹œ
                q_values = self.q_table[state]
                action = self.choose_action(state)
                
                # Action names for display
                action_names = ["â¬…ï¸ Left", "â¬‡ï¸ Down", "â¡ï¸ Right", "â¬†ï¸ Up"]
                action_symbols = ["â†", "â†“", "â†’", "â†‘"]
                
                if verbose:
                    print(f"ğŸ“ Step {steps + 1}:")
                    print(f"   í˜„ì¬ ìƒíƒœ: {state} (ìœ„ì¹˜: {state//4}, {state%4})")
                    print(f"   Q-ê°’ë“¤: {q_values}")
                    print(f"   ì„ íƒí•œ í–‰ë™: {action} ({action_names[action]})")
                
                # í–‰ë™ ì‹¤í–‰
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                # ê²½ë¡œì— ì¶”ê°€
                path.append(next_state)
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state = next_state
                total_reward += reward
                steps += 1
                
                # ë Œë”ë§
                if self.render_mode == "human":
                    env.render()
                    time.sleep(step_delay)
                
                if verbose:
                    print(f"   â¡ï¸ ë‹¤ìŒ ìƒíƒœ: {state} (ìœ„ì¹˜: {state//4}, {state%4})")
                    print(f"   ğŸ ë³´ìƒ: {reward}")
                    
                    if done:
                        if reward > 0:
                            print("   ğŸ‰ ê³¨ ë„ë‹¬!")
                        else:
                            print("   ğŸ’€ êµ¬ë©ì— ë¹ ì§!")
                    print()
                
                if done:
                    break
            
            # ì—í”¼ì†Œë“œ ìš”ì•½
            result = "âœ… ì„±ê³µ" if total_reward > 0 else "âŒ ì‹¤íŒ¨"
            if total_reward > 0:
                successes += 1
                
            print(f"ğŸ“Š ì—í”¼ì†Œë“œ ê²°ê³¼:")
            print(f"   ê²°ê³¼: {result}")
            print(f"   ì´ ë³´ìƒ: {total_reward}")
            print(f"   ì†Œìš” ìŠ¤í…: {steps}")
            print(f"   ì´ë™ ê²½ë¡œ: {' â†’ '.join(map(str, path))}")
            print(f"   í˜„ì¬ê¹Œì§€ ì„±ê³µë¥ : {successes}/{episode+1} ({successes/(episode+1)*100:.1f}%)")
            print()
            
            # ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ì§„í–‰ í™•ì¸
            if episode < num_episodes - 1:
                if interactive:
                    input("â¸ï¸  Press Enter to continue to next episode...")
                else:
                    time.sleep(2)  # ìë™ìœ¼ë¡œ 2ì´ˆ í›„ ì§„í–‰
                print()
        
        # ìµœì¢… ìš”ì•½
        print("ğŸ† ì‹œì—° ì™„ë£Œ!")
        print(f"   ì´ ì„±ê³µë¥ : {successes}/{num_episodes} ({successes/num_episodes*100:.1f}%)")
        
    def watch_agent_play(self, env, num_episodes=5, step_delay=1.0, show_q_values=True):
        """
        ì—ì´ì „íŠ¸ì˜ í”Œë ˆì´ë¥¼ ìì„¸íˆ ê´€ì°°í•˜ëŠ” í•¨ìˆ˜
        """
        print("ğŸ‘€ ì—ì´ì „íŠ¸ í”Œë ˆì´ ê´€ì°° ëª¨ë“œ")
        print("Q-ê°’ê³¼ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ ìì„¸íˆ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        print()
        
        for episode in range(num_episodes):
            print(f"ğŸ¯ ê´€ì°° ì—í”¼ì†Œë“œ {episode + 1}/{num_episodes}")
            print("-" * 50)
            
            state, _ = env.reset()
            done = False
            step = 0
            
            while not done and step < 100:
                if show_q_values:
                    q_values = self.q_table[state]
                    print(f"ğŸ“Š ìƒíƒœ {state}ì˜ Q-ê°’ ë¶„ì„:")
                    actions = ["Left", "Down", "Right", "Up"]
                    for i, (action, q_val) in enumerate(zip(actions, q_values)):
                        marker = "ğŸ†" if q_val == np.max(q_values) else "  "
                        print(f"   {marker} {action:>5}: {q_val:6.3f}")
                
                action = self.choose_action(state)
                actions = ["â¬…ï¸", "â¬‡ï¸", "â¡ï¸", "â¬†ï¸"]
                print(f"ğŸ¯ ì„ íƒëœ í–‰ë™: {actions[action]}")
                
                if self.render_mode == "human":
                    env.render()
                    time.sleep(step_delay)
                
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                print(f"ğŸ“ {state} â†’ {next_state}, ë³´ìƒ: {reward}")
                
                state = next_state
                step += 1
                
                if done:
                    result = "ğŸ‰ ì„±ê³µ!" if reward > 0 else "ğŸ’€ ì‹¤íŒ¨"
                    print(f"ğŸ ê²Œì„ ì¢…ë£Œ: {result}")
                
                print()
            
            if episode < num_episodes - 1:
                input("ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
                print()


def main():
    parser = argparse.ArgumentParser(description="FrozenLake Q-Learning í‰ê°€")
    
    # í™˜ê²½ íŒŒë¼ë¯¸í„°
    parser.add_argument("--map", choices=["4x4", "8x8"], default="4x4",
                       help="ë§µ í¬ê¸° (ê¸°ë³¸ê°’: 4x4)")
    parser.add_argument("--slippery", action="store_true", default=True,
                       help="ë¯¸ë„ëŸ¬ìš´ í‘œë©´ í™œì„±í™” (ê¸°ë³¸ê°’: True)")
    parser.add_argument("--no-slippery", action="store_false", dest="slippery",
                       help="ë¯¸ë„ëŸ¬ìš´ í‘œë©´ ë¹„í™œì„±í™”")
    
    # í‰ê°€ íŒŒë¼ë¯¸í„°
    parser.add_argument("--episodes", type=int, default=100,
                       help="í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜ (default: 100)")
    parser.add_argument("--max-steps", type=int, default=100,
                       help="ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜ (default: 100)")
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„°
    parser.add_argument("--load-path", type=str, default="weights/q_table_4x4.npy",
                       help="Q-table ë¡œë“œ ê²½ë¡œ (ê¸°ë³¸ê°’: weights/q_table_4x4.npy)")
    
    # ë Œë”ë§ íŒŒë¼ë¯¸í„°
    parser.add_argument("--render", choices=["human", "none"], default="none",
                       help="ë Œë”ë§ ëª¨ë“œ (ê¸°ë³¸ê°’: none)")
    parser.add_argument("--demonstrate", action="store_true",
                       help="ì‹œê°ì  ë Œë”ë§ìœ¼ë¡œ ì •ì±… ì‹œì—°")
    parser.add_argument("--demo-episodes", type=int, default=3,
                       help="ì‹œì—°í•  ì—í”¼ì†Œë“œ ìˆ˜ (default: 3)")
    parser.add_argument("--step-delay", type=float, default=0.8,
                       help="ì‹œì—° ì‹œ ìŠ¤í… ê°„ ì§€ì—° ì‹œê°„ (ê¸°ë³¸ê°’: 0.8)")
    parser.add_argument("--watch-mode", action="store_true",
                       help="ìƒì„¸í•œ Q-ê°’ ë¶„ì„ì´ í¬í•¨ëœ ê´€ì°° ëª¨ë“œ")
    parser.add_argument("--auto-play", action="store_true",
                       help="ìë™ ì¬ìƒ ëª¨ë“œ (ì—í”¼ì†Œë“œ ê°„ ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ)")
    
    # ê¸°íƒ€ íŒŒë¼ë¯¸í„°
    parser.add_argument("--seed", type=int, default=42,
                       help="ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)")
    parser.add_argument("--quiet", action="store_true",
                       help="í‰ê°€ ì§„í–‰ ìƒí™© ì¶œë ¥ ì–µì œ")
    
    args = parser.parse_args()
    
    # ëœë¤ ì‹œë“œ ì„¤ì •
    np.random.seed(args.seed)
    
    # TODO: í•™ìŠµëœ Q-tableì„ ë¡œë“œí•˜ì„¸ìš”
    # íŒíŠ¸ 1: utils.ioì˜ load_q_table() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
    # íŒíŠ¸ 2: args.load_pathë¥¼ ì¸ìë¡œ ì „ë‹¬í•©ë‹ˆë‹¤
    # íŒíŠ¸ 3: FileNotFoundError ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤
    # íŒíŠ¸ 4: íŒŒì¼ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  sys.exit(1)ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤
    #YOUR CODE HERE
    raise NotImplementedError("Q-table ë¡œë“œë¥¼ êµ¬í˜„í•˜ì„¸ìš”")
    
    # í˜¸í™˜ì„± í™•ì¸ì„ ìœ„í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ ì‹œë„
    hyperparams_path = args.load_path.replace('.npy', '_hyperparams.json')
    try:
        hyperparams = load_hyperparameters(hyperparams_path)
        
        # í˜¸í™˜ì„± í™•ì¸
        if hyperparams.get('map_name') != args.map:
            print(f"Warning: Loaded model was trained on {hyperparams.get('map_name')} "
                  f"but evaluating on {args.map}")
        if hyperparams.get('is_slippery') != args.slippery:
            print(f"Warning: Loaded model was trained with slippery={hyperparams.get('is_slippery')} "
                  f"but evaluating with slippery={args.slippery}")
    except FileNotFoundError:
        print("Warning: Could not load hyperparameters file. Proceeding with evaluation...")
    
    # ë Œë” ëª¨ë“œ ê²°ì •
    render_mode = args.render if args.render != "none" else None
    if args.demonstrate and render_mode != "human":
        render_mode = "human"
        print("ì‹œì—°ì„ ìœ„í•´ render_modeë¥¼ 'human'ìœ¼ë¡œ ì„¤ì •")
    
    # í™˜ê²½ ìƒì„±
    env = gym.make("FrozenLake-v1", 
                   map_name=args.map, 
                   is_slippery=args.slippery,
                   render_mode=render_mode)
    
    # í‰ê°€ì ìƒì„±
    evaluator = QLearningEvaluator(q_table, render_mode=render_mode)
    
    # í‰ê°€ ì‹¤í–‰
    if not args.quiet:
        print(f"ë¡œë“œëœ Q-table í˜•íƒœ: {q_table.shape}")
        print(f"í™˜ê²½ ìƒíƒœ ê³µê°„: {env.observation_space.n}")
        print(f"í™˜ê²½ í–‰ë™ ê³µê°„: {env.action_space.n}")
        print()
    
    # í‘œì¤€ í‰ê°€
    eval_stats = evaluator.evaluate_episodes(
        env=env,
        num_episodes=args.episodes,
        max_steps_per_episode=args.max_steps,
        verbose=not args.quiet
    )
    
    # ì‹œì—° ëª¨ë“œ
    if args.demonstrate:
        print()
        if args.watch_mode:
            evaluator.watch_agent_play(
                env=env,
                num_episodes=args.demo_episodes,
                step_delay=args.step_delay,
                show_q_values=True
            )
        else:
            evaluator.demonstrate_policy(
                env=env,
                num_episodes=args.demo_episodes,
                step_delay=args.step_delay,
                verbose=not args.quiet,
                interactive=not args.auto_play
            )
    
    env.close()
    
    if not args.quiet:
        print("\nEvaluation completed!")


if __name__ == "__main__":
    main()