{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C (Advantage Actor-Critic) 튜토리얼\n",
    "\n",
    "이 노트북에서는 A2C (Advantage Actor-Critic) 알고리즘의 핵심 개념을 단계별로 학습합니다.\n",
    "\n",
    "## 목차\n",
    "1. Actor-Critic의 등장 배경\n",
    "2. 배우(Actor)와 평론가(Critic)의 역할\n",
    "3. 어드밴티지 함수 (Advantage Function)\n",
    "4. A2C 신경망 아키텍처\n",
    "5. Actor와 Critic의 손실 함수\n",
    "6. 전체 학습 루프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Actor-Critic의 등장 배경\n",
    "\n",
    "강화학습에는 크게 두 가지 접근법이 있습니다:\n",
    "\n",
    "### 정책 기반 방법 (Policy-based methods, e.g., REINFORCE)\n",
    "\n",
    "**장점:**\n",
    "- 연속적인 행동 공간에서도 작동\n",
    "- 확률적 정책 학습 가능\n",
    "\n",
    "**단점:**\n",
    "- **높은 분산(High Variance)**: 같은 상태에서도 매번 다른 보상을 받을 수 있어 학습이 불안정\n",
    "- 샘플 효율성이 낮음\n",
    "\n",
    "```python\n",
    "# REINFORCE의 정책 gradient\n",
    "# J(θ) = E[Σ log π(a|s) * R]  # R은 총 보상 (분산이 매우 큼!)\n",
    "```\n",
    "\n",
    "### 가치 기반 방법 (Value-based methods, e.g., DQN)\n",
    "\n",
    "**장점:**\n",
    "- 분산이 낮아 안정적 학습\n",
    "- 샘플 효율성이 높음\n",
    "\n",
    "**단점:**\n",
    "- **이산적 행동 공간만 가능**: 연속적인 행동을 다루기 어려움\n",
    "- 확률적 정책을 학습할 수 없음 (deterministic)\n",
    "\n",
    "### 🎯 Actor-Critic의 해결책\n",
    "\n",
    "Actor-Critic은 **두 방법의 장점을 결합**합니다:\n",
    "- **Actor**: 정책 기반 (연속 행동 가능)\n",
    "- **Critic**: 가치 기반 (분산 감소)\n",
    "\n",
    "→ **Critic의 가치 함수를 baseline으로 사용하여 분산을 줄이면서도 정책을 직접 학습!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 배우(Actor)와 평론가(Critic)의 역할\n",
    "\n",
    "### 🎭 Actor (배우)\n",
    "\n",
    "**역할**: \"어떤 행동을 할지\" 결정하는 정책 학습\n",
    "\n",
    "```python\n",
    "# Actor는 정책 π(a|s)를 출력\n",
    "action_probs = actor(state)  # [0.1, 0.3, 0.6] - 각 행동의 확률\n",
    "action = sample(action_probs)  # 확률에 따라 행동 선택\n",
    "```\n",
    "\n",
    "**수식**: π(a|s; θ) - 상태 s에서 행동 a를 선택할 확률\n",
    "\n",
    "### 👨‍⚖️ Critic (평론가)\n",
    "\n",
    "**역할**: \"현재 상태가 얼마나 좋은지\" 평가하는 가치 함수 학습\n",
    "\n",
    "```python\n",
    "# Critic은 상태 가치 V(s)를 출력\n",
    "state_value = critic(state)  # 15.3 - 이 상태의 예상 총 보상\n",
    "```\n",
    "\n",
    "**수식**: V(s; w) - 상태 s의 가치 (기대 누적 보상)\n",
    "\n",
    "### 🤝 둘의 상호작용\n",
    "\n",
    "**비유로 이해하기:**\n",
    "\n",
    "마리오 게임을 배우는 상황을 상상해봅시다.\n",
    "\n",
    "- **Actor (배우)**: \"여기서 점프해야겠어!\" → 행동 실행\n",
    "- **Critic (평론가)**: \"음... 이 위치는 위험해 보이는데? (낮은 점수)\" → 평가\n",
    "- **Actor**: \"아, 평론가가 낮게 평가했네. 다음엔 다른 행동을 시도해야겠다\" → 학습\n",
    "\n",
    "→ Critic의 피드백으로 Actor가 더 나은 행동을 학습!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 어드밴티지 함수 (Advantage Function)\n",
    "\n",
    "### 🤔 문제: 절대적 가치 vs 상대적 가치\n",
    "\n",
    "Q-value가 높다고 해서 그 행동이 \"좋은\" 행동일까요?\n",
    "\n",
    "**예시:**\n",
    "```\n",
    "상태 A:\n",
    "  - 왼쪽으로 가기: Q(s, left) = 10\n",
    "  - 오른쪽으로 가기: Q(s, right) = 15\n",
    "  - V(s) = 12.5 (평균)\n",
    "\n",
    "→ 오른쪽이 \"절대적으로\" 좋다? (15점)\n",
    "→ 아니면 \"평균보다\" 2.5점 더 좋다?\n",
    "```\n",
    "\n",
    "### 💡 Advantage Function의 아이디어\n",
    "\n",
    "**\"평균보다 얼마나 더 좋은가?\"**를 측정합니다.\n",
    "\n",
    "```\n",
    "A(s, a) = Q(s, a) - V(s)\n",
    "```\n",
    "\n",
    "**의미:**\n",
    "- A(s, a) > 0: 평균보다 좋은 행동 → 더 자주 선택하도록 학습\n",
    "- A(s, a) < 0: 평균보다 나쁜 행동 → 덜 선택하도록 학습\n",
    "- A(s, a) = 0: 평균적인 행동\n",
    "\n",
    "### 🎯 분산 감소 원리\n",
    "\n",
    "**REINFORCE (baseline 없음):**\n",
    "```python\n",
    "# 총 보상 R은 매우 불안정 (분산 큼)\n",
    "policy_gradient = log(π(a|s)) * R\n",
    "```\n",
    "\n",
    "**Actor-Critic (baseline 사용):**\n",
    "```python\n",
    "# V(s)를 빼서 분산 감소!\n",
    "policy_gradient = log(π(a|s)) * (R - V(s))\n",
    "policy_gradient = log(π(a|s)) * A(s, a)  # Advantage!\n",
    "```\n",
    "\n",
    "### 📊 실제 계산 방법\n",
    "\n",
    "Q(s,a)를 직접 계산하는 대신, **TD error**를 사용합니다:\n",
    "\n",
    "```python\n",
    "# TD error = 실제 받은 보상 + 다음 상태 가치 - 현재 상태 가치\n",
    "td_error = reward + γ * V(s') - V(s)\n",
    "\n",
    "# TD error가 Advantage의 추정치!\n",
    "A(s, a) ≈ td_error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. A2C 신경망 아키텍처 (PyTorch)\n",
    "\n",
    "### 🏗️ 전체 구조\n",
    "\n",
    "```\n",
    "입력 (게임 화면)\n",
    "    ↓\n",
    "공통 CNN (Feature Extractor)\n",
    "    ↓\n",
    "LSTM (시간적 패턴 학습)\n",
    "    ↓\n",
    "  ┌─────────┐\n",
    "  │         │\n",
    "Actor     Critic\n",
    "(정책)    (가치)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    A2C 신경망: 공통 백본 + Actor/Critic 헤드\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # === 공통 Feature Extractor (CNN) ===\n",
    "        # 게임 화면에서 중요한 특징을 추출\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        \n",
    "        # === LSTM (시간적 패턴 학습) ===\n",
    "        # \"이전에 어떤 상황이었는지\" 기억\n",
    "        self.lstm = nn.LSTMCell(32 * 6 * 6, 512)\n",
    "        \n",
    "        # === Actor Head (정책 네트워크) ===\n",
    "        # 출력: 각 행동의 점수 (logits)\n",
    "        self.actor_linear = nn.Linear(512, num_actions)\n",
    "        \n",
    "        # === Critic Head (가치 네트워크) ===\n",
    "        # 출력: 현재 상태의 가치 (scalar)\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, x, hx, cx):\n",
    "        # CNN으로 특징 추출\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # LSTM 통과\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "        \n",
    "        # Actor와 Critic 출력\n",
    "        actor_logits = self.actor_linear(hx)    # [batch, num_actions]\n",
    "        critic_value = self.critic_linear(hx)   # [batch, 1]\n",
    "        \n",
    "        return actor_logits, critic_value, hx, cx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 각 레이어의 역할\n",
    "\n",
    "#### 1. CNN Layers (conv1~4)\n",
    "- **입력**: 게임 화면 (84x84 grayscale)\n",
    "- **역할**: 적, 블록, 마리오 위치 등 중요한 시각적 패턴 학습\n",
    "- **출력**: 고수준 특징 벡터\n",
    "\n",
    "#### 2. LSTM Layer\n",
    "- **입력**: CNN 특징 벡터\n",
    "- **역할**: 시간적 패턴 학습 (예: \"방금 점프했으니 다음엔 착지할 것\")\n",
    "- **출력**: 시간 정보를 포함한 특징 벡터 (512차원)\n",
    "\n",
    "#### 3. Actor Head\n",
    "- **입력**: LSTM 출력 (512차원)\n",
    "- **출력**: 각 행동의 logits (12개 행동)\n",
    "- **의미**: \"각 행동이 얼마나 좋아 보이는가\"\n",
    "\n",
    "```python\n",
    "# Softmax를 통해 확률 분포로 변환\n",
    "logits = actor_head(features)      # [2.1, 0.5, -1.3, ...]\n",
    "probs = softmax(logits)            # [0.5, 0.2, 0.1, ...] (합=1)\n",
    "action = sample(probs)             # 확률에 따라 행동 선택\n",
    "```\n",
    "\n",
    "#### 4. Critic Head\n",
    "- **입력**: LSTM 출력 (512차원)\n",
    "- **출력**: 상태 가치 V(s) (1개 스칼라)\n",
    "- **의미**: \"지금 상황에서 앞으로 얻을 총 보상의 기댓값\"\n",
    "\n",
    "```python\n",
    "value = critic_head(features)  # 15.3 (이 상태의 예상 총 보상)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Actor와 Critic의 손실 함수\n",
    "\n",
    "### 🎯 Actor Loss (정책 손실)\n",
    "\n",
    "**목표**: Advantage가 양수인 행동은 더 자주, 음수인 행동은 덜 자주 선택하도록 학습\n",
    "\n",
    "**수식:**\n",
    "```\n",
    "L_actor = -log(π(a|s)) * A(s,a).detach()\n",
    "```\n",
    "\n",
    "**직관적 이해:**\n",
    "\n",
    "```python\n",
    "# 예시 1: Advantage가 양수 (좋은 행동)\n",
    "log_prob = -2.0  # log(π(a|s))\n",
    "advantage = +5.0  # 평균보다 훨씬 좋음!\n",
    "actor_loss = -(-2.0) * 5.0 = +10.0\n",
    "\n",
    "# Gradient descent로 loss 감소\n",
    "# → log_prob 증가 → π(a|s) 증가 → 이 행동을 더 자주 선택!\n",
    "\n",
    "# 예시 2: Advantage가 음수 (나쁜 행동)\n",
    "log_prob = -2.0\n",
    "advantage = -3.0  # 평균보다 나쁨\n",
    "actor_loss = -(-2.0) * (-3.0) = -6.0\n",
    "\n",
    "# Gradient descent로 loss 감소\n",
    "# → log_prob 감소 → π(a|s) 감소 → 이 행동을 덜 선택!\n",
    "```\n",
    "\n",
    "**❗ 중요: `.detach()`를 사용하는 이유**\n",
    "\n",
    "Advantage는 Critic으로부터 계산되지만, Actor 업데이트 시에는 **Critic을 고정**해야 합니다.\n",
    "\n",
    "```python\n",
    "# Advantage 계산\n",
    "advantage = td_error.detach()  # Critic gradient를 차단!\n",
    "\n",
    "# Actor만 업데이트\n",
    "actor_loss = -log_prob * advantage\n",
    "```\n",
    "\n",
    "### 📊 Critic Loss (가치 손실)\n",
    "\n",
    "**목표**: 실제 받은 보상과 예측한 가치의 차이를 최소화\n",
    "\n",
    "**수식:**\n",
    "```\n",
    "L_critic = (Return - V(s))²\n",
    "```\n",
    "\n",
    "**직관적 이해:**\n",
    "\n",
    "```python\n",
    "# 상황: 마리오가 적을 밟고 100점 획득\n",
    "predicted_value = 50.0    # Critic의 예측\n",
    "actual_return = 100.0     # 실제로 받은 보상\n",
    "\n",
    "critic_loss = (100.0 - 50.0)² = 2500.0  # 큰 오차!\n",
    "\n",
    "# Gradient descent\n",
    "# → predicted_value를 100.0에 가깝게 조정\n",
    "# → 다음엔 이런 상황에서 더 정확하게 예측!\n",
    "```\n",
    "\n",
    "### 🎲 Entropy Loss (탐험 보너스)\n",
    "\n",
    "**목표**: 너무 확신 있는 정책을 방지하고 탐험 장려\n",
    "\n",
    "**수식:**\n",
    "```\n",
    "H(π) = -Σ π(a|s) * log(π(a|s))\n",
    "L_entropy = -β * H(π)  # β는 entropy coefficient\n",
    "```\n",
    "\n",
    "**직관적 이해:**\n",
    "\n",
    "```python\n",
    "# 나쁜 경우: 너무 확신 있는 정책 (탐험 부족)\n",
    "probs = [0.99, 0.01, 0.00, 0.00]  # 거의 한 가지 행동만!\n",
    "entropy = -0.99*log(0.99) - 0.01*log(0.01) = 0.08  # 낮은 엔트로피\n",
    "\n",
    "# 좋은 경우: 다양한 행동 시도 (탐험 충분)\n",
    "probs = [0.3, 0.3, 0.2, 0.2]  # 여러 행동을 고려\n",
    "entropy = -(4개 확률의 합) = 1.28  # 높은 엔트로피\n",
    "\n",
    "# Entropy를 최대화 (loss는 음수)\n",
    "# → 다양한 행동을 시도하도록 유도!\n",
    "```\n",
    "\n",
    "### 🔀 Total Loss (통합 손실)\n",
    "\n",
    "```python\n",
    "total_loss = -actor_loss + critic_loss - β * entropy_loss\n",
    "```\n",
    "\n",
    "**각 항의 역할:**\n",
    "- **-actor_loss**: 좋은 행동을 더 자주 선택 (정책 개선)\n",
    "- **+critic_loss**: 가치 예측을 정확하게 (안정적 학습)\n",
    "- **-β * entropy_loss**: 탐험을 장려 (지역 최적화 방지)\n",
    "\n",
    "**β (beta) 하이퍼파라미터:**\n",
    "- β가 크면: 더 많은 탐험 (초반에 유리)\n",
    "- β가 작으면: 더 적극적인 활용 (후반에 유리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 전체 학습 루프 (Training Loop) 해부\n",
    "\n",
    "### 📋 학습 과정 개요\n",
    "\n",
    "```\n",
    "1. 경험 수집 (Experience Collection)\n",
    "   ↓\n",
    "2. Advantage 계산 (Advantage Estimation)\n",
    "   ↓\n",
    "3. 손실 계산 (Loss Computation)\n",
    "   ↓\n",
    "4. 역전파 (Backpropagation)\n",
    "   ↓\n",
    "5. 반복\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: 경험 수집 (n-step 동안) ===\n",
    "\n",
    "num_steps = 50  # n-step\n",
    "log_policies = []\n",
    "values = []\n",
    "rewards = []\n",
    "entropies = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Forward pass\n",
    "    logits, value, hx, cx = model(state, hx, cx)\n",
    "    \n",
    "    # 정책 확률 계산\n",
    "    policy = F.softmax(logits, dim=1)        # [0.3, 0.2, 0.5, ...]\n",
    "    log_policy = F.log_softmax(logits, dim=1)  # [log(0.3), log(0.2), ...]\n",
    "    \n",
    "    # Entropy 계산 (탐험 장려)\n",
    "    entropy = -(policy * log_policy).sum(1, keepdim=True)\n",
    "    \n",
    "    # 행동 샘플링\n",
    "    m = torch.distributions.Categorical(policy)\n",
    "    action = m.sample().item()  # 확률에 따라 행동 선택\n",
    "    \n",
    "    # 환경에서 한 스텝 실행\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # 경험 저장\n",
    "    values.append(value)\n",
    "    log_policies.append(log_policy[0, action])  # 선택한 행동의 log prob\n",
    "    rewards.append(reward)\n",
    "    entropies.append(entropy)\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 2: Bootstrap Value (에피소드가 끝나지 않았을 때) ===\n",
    "\n",
    "R = torch.zeros(1, 1)\n",
    "if not done:\n",
    "    # 마지막 상태의 가치로 초기화\n",
    "    _, R, _, _ = model(state, hx, cx)\n",
    "    # 의미: \"앞으로 받을 보상의 예측값\"\n",
    "\n",
    "# === STEP 3: Advantage 계산 (GAE - Generalized Advantage Estimation) ===\n",
    "\n",
    "gae = torch.zeros(1, 1)  # GAE 누적값\n",
    "actor_loss = 0\n",
    "critic_loss = 0\n",
    "entropy_loss = 0\n",
    "next_value = R\n",
    "\n",
    "# 역순으로 계산 (뒤에서부터)\n",
    "for value, log_policy, reward, entropy in reversed(list(zip(values, log_policies, rewards, entropies))):\n",
    "    # GAE 계산\n",
    "    # gae = δ_t + γ*λ*gae_{t+1}\n",
    "    # δ_t = r_t + γ*V(s_{t+1}) - V(s_t)  (TD error)\n",
    "    gae = gae * gamma * tau\n",
    "    gae = gae + reward + gamma * next_value.detach() - value.detach()\n",
    "    next_value = value\n",
    "    \n",
    "    # Actor loss (정책 gradient)\n",
    "    actor_loss = actor_loss + log_policy * gae  # 주목! gae가 advantage\n",
    "    \n",
    "    # Return 계산 (n-step return)\n",
    "    R = R * gamma + reward\n",
    "    \n",
    "    # Critic loss (MSE)\n",
    "    critic_loss = critic_loss + (R - value) ** 2 / 2\n",
    "    \n",
    "    # Entropy loss\n",
    "    entropy_loss = entropy_loss + entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 4: Total Loss 계산 및 역전파 ===\n",
    "\n",
    "total_loss = -actor_loss + critic_loss - beta * entropy_loss\n",
    "\n",
    "# Optimizer 초기화\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 역전파\n",
    "total_loss.backward()\n",
    "\n",
    "# Gradient clipping (안정적 학습)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "# Parameter 업데이트\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Actor Loss: {-actor_loss.item():.4f}\")\n",
    "print(f\"Critic Loss: {critic_loss.item():.4f}\")\n",
    "print(f\"Entropy: {entropy_loss.item():.4f}\")\n",
    "print(f\"Total Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 학습 루프의 핵심 포인트\n",
    "\n",
    "#### 1. N-step Experience Collection\n",
    "- 한 번에 여러 스텝의 경험을 모음\n",
    "- 배치 학습의 효율성\n",
    "\n",
    "#### 2. GAE (Generalized Advantage Estimation)\n",
    "```python\n",
    "# GAE는 여러 시간 스텝의 TD error를 조합\n",
    "gae = δ_t + γ*λ*δ_{t+1} + (γ*λ)²*δ_{t+2} + ...\n",
    "\n",
    "# λ=0: 1-step TD (낮은 분산, 높은 편향)\n",
    "# λ=1: Monte Carlo (높은 분산, 낮은 편향)\n",
    "# λ=0.95: 좋은 균형점\n",
    "```\n",
    "\n",
    "#### 3. Gradient Clipping\n",
    "```python\n",
    "# Gradient가 너무 크면 학습이 불안정\n",
    "# → 일정 크기로 제한\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "```\n",
    "\n",
    "#### 4. `.detach()` 사용\n",
    "```python\n",
    "# Critic의 gradient가 Actor로 흐르지 않도록 차단\n",
    "advantage = (reward + gamma * next_value.detach() - value.detach())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎓 요약 및 핵심 개념\n",
    "\n",
    "### A2C의 핵심 아이디어\n",
    "\n",
    "1. **Actor-Critic 분리**\n",
    "   - Actor: 정책 학습 (어떤 행동을 할지)\n",
    "   - Critic: 가치 학습 (상황이 얼마나 좋은지)\n",
    "\n",
    "2. **Advantage Function**\n",
    "   - A(s,a) = Q(s,a) - V(s)\n",
    "   - \"평균보다 얼마나 좋은가\"\n",
    "   - 분산 감소 효과\n",
    "\n",
    "3. **통합 손실 함수**\n",
    "   ```\n",
    "   L = -L_actor + L_critic - β*L_entropy\n",
    "   ```\n",
    "   - 정책 개선 + 가치 예측 + 탐험\n",
    "\n",
    "4. **안정적 학습을 위한 테크닉**\n",
    "   - Gradient clipping\n",
    "   - `.detach()` for advantage\n",
    "   - GAE for better advantage estimation\n",
    "\n",
    "### 🚀 다음 스텝\n",
    "\n",
    "이제 실제로 학습을 실행해봅시다!\n",
    "\n",
    "```bash\n",
    "# 학습 시작\n",
    "python training/train_a2c.py --num-updates 10000\n",
    "\n",
    "# 학습된 모델 데모\n",
    "python games/demo_trained_agent.py --model-path models/saved_weights/mario_a3c_XXX.pth\n",
    "```\n",
    "\n",
    "### 📚 추가 학습 자료\n",
    "\n",
    "- [Original A3C Paper](https://arxiv.org/abs/1602.01783)\n",
    "- [GAE Paper](https://arxiv.org/abs/1506.02438)\n",
    "- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/vpg.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🧪 실습: 간단한 환경에서 A2C 테스트\n",
    "\n",
    "CartPole 환경에서 A2C가 어떻게 작동하는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 간단한 A2C 모델 (CartPole용)\n",
    "class SimpleActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, num_actions):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "        self.actor = nn.Linear(128, num_actions)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc(x))\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "# 환경 생성\n",
    "env = gym.make('CartPole-v1')\n",
    "model = SimpleActorCritic(4, 2)  # 상태 4차원, 행동 2개\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"간단한 A2C 모델 준비 완료!\")\n",
    "print(f\"상태 차원: {env.observation_space.shape[0]}\")\n",
    "print(f\"행동 개수: {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 (간단 버전)\n",
    "episode_rewards = []\n",
    "num_episodes = 500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    \n",
    "    # 에피소드 진행\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        logits, value = model(state_tensor)\n",
    "        \n",
    "        policy = F.softmax(logits, dim=1)\n",
    "        m = Categorical(policy)\n",
    "        action = m.sample()\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        log_probs.append(m.log_prob(action))\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    # 손실 계산 및 업데이트\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + 0.99 * R\n",
    "        returns.insert(0, R)\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    values = torch.cat(values)\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    \n",
    "    advantage = returns - values.detach()\n",
    "    \n",
    "    actor_loss = -(log_probs * advantage).mean()\n",
    "    critic_loss = F.mse_loss(values.squeeze(), returns)\n",
    "    \n",
    "    loss = actor_loss + critic_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-50:])\n",
    "        print(f\"Episode {episode}: Avg Reward = {avg_reward:.2f}\")\n",
    "\n",
    "print(\"학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Moving average\n",
    "window = 50\n",
    "moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(episode_rewards, alpha=0.3, label='Episode Reward')\n",
    "plt.plot(moving_avg, label=f'{window}-Episode Moving Average')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('A2C Learning Curve on CartPole')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"최종 100 에피소드 평균 보상: {np.mean(episode_rewards[-100:]):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
