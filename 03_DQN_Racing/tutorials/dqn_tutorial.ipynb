{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN (Deep Q-Networks) Tutorial\n",
    "\n",
    "Welcome to the comprehensive DQN tutorial! This notebook will guide you through all the essential components of Deep Q-Networks for reinforcement learning.\n",
    "\n",
    "## üìö What You'll Learn\n",
    "1. **Q-Learning Review and Limitations** - Why neural networks are needed\n",
    "2. **Neural Network Architecture** - CNN design for visual inputs\n",
    "3. **Experience Replay Buffer** - Breaking correlation in training data\n",
    "4. **Target Network Mechanism** - Stabilizing Q-learning\n",
    "5. **Epsilon-Greedy Strategy** - Balancing exploration and exploitation\n",
    "6. **Loss Function and Optimization** - Training the neural network\n",
    "\n",
    "## üöÄ Getting Started\n",
    "Make sure you have activated the virtual environment and installed all dependencies:\n",
    "```bash\n",
    "source dqn_racing_env/bin/activate  # or activate_env.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from collections import deque\n",
    "import cv2\n",
    "from typing import Tuple, List, Optional\n",
    "import time\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "print(\"üéØ DQN Tutorial Environment Setup Complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Q-Learning Review and Limitations\n",
    "\n",
    "Let's start by understanding why we need Deep Q-Networks instead of traditional Q-Tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningDemo:\n",
    "    \"\"\"Demonstrates Q-Learning limitations with large state spaces.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.q_table_size_examples = {\n",
    "            \"GridWorld 4x4\": 16,\n",
    "            \"GridWorld 10x10\": 100,\n",
    "            \"Atari (84x84 grayscale)\": 256**(84*84),\n",
    "            \"CarRacing (84x84x3)\": 256**(84*84*3)\n",
    "        }\n",
    "        \n",
    "    def demonstrate_state_space_explosion(self):\n",
    "        \"\"\"Show how state space grows exponentially.\"\"\"\n",
    "        print(\"Q-Table size for different environments:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for env_name, size in self.q_table_size_examples.items():\n",
    "            if size < 1e6:\n",
    "                print(f\"{env_name:<25}: {size:,} states\")\n",
    "            else:\n",
    "                print(f\"{env_name:<25}: {size:.2e} states (IMPOSSIBLE!)\")\n",
    "                \n",
    "        print(\"\\nüîë Key Insights:\")\n",
    "        print(\"üìä Q-Tables work for small, discrete state spaces\")\n",
    "        print(\"‚ùå Q-Tables fail for large, continuous, or high-dimensional states\")\n",
    "        print(\"‚úÖ Neural Networks can approximate Q-functions for complex states\")\n",
    "\n",
    "# Run the demonstration\n",
    "demo = QLearningDemo()\n",
    "demo.demonstrate_state_space_explosion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Why This Matters\n",
    "\n",
    "The CarRacing environment has **continuous visual input** (84x84x3 pixels). A Q-table would need to store values for every possible pixel combination - that's more states than atoms in the observable universe!\n",
    "\n",
    "**Solution**: Use a neural network to approximate the Q-function: `Q(s,a) ‚âà Q_Œ∏(s,a)`\n",
    "\n",
    "---\n",
    "# Section 2: Neural Network Architecture\n",
    "\n",
    "Let's design a CNN-based network for processing visual input from CarRacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"CNN-based Deep Q-Network for CarRacing environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_dim: int = 3, input_channels: int = 4):\n",
    "        \"\"\"\n",
    "        Initialize DQN network.\n",
    "        \n",
    "        Args:\n",
    "            action_dim: Number of possible actions\n",
    "            input_channels: Number of input channels (frame stack)\n",
    "        \"\"\"\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        \n",
    "        # Convolutional layers for feature extraction\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Calculate size of flattened features\n",
    "        self._conv_output_size = self._get_conv_output_size((input_channels, 84, 84))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._conv_output_size, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _get_conv_output_size(self, input_shape: Tuple[int, int, int]) -> int:\n",
    "        \"\"\"Calculate the output size after convolutional layers.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            dummy_output = self._forward_conv(dummy_input)\n",
    "            return dummy_output.numel()\n",
    "            \n",
    "    def _forward_conv(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through convolutional layers only.\"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return x.flatten(1)\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier initialization.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "                    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, channels, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Q-values for each action\n",
    "        \"\"\"\n",
    "        # Convolutional feature extraction\n",
    "        x = self._forward_conv(x)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def get_network_info(self):\n",
    "        \"\"\"Get information about the network architecture.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        info = {\n",
    "            \"total_parameters\": total_params,\n",
    "            \"conv_output_size\": self._conv_output_size,\n",
    "            \"input_shape\": (4, 84, 84),\n",
    "            \"output_shape\": 3\n",
    "        }\n",
    "        return info\n",
    "\n",
    "# Create and analyze the network\n",
    "dqn = DQNNetwork(action_dim=3, input_channels=4)\n",
    "info = dqn.get_network_info()\n",
    "\n",
    "print(\"üß† DQN Network Architecture:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Input: {info['input_shape']} (4 stacked frames of 84x84)\")\n",
    "print(\"Conv1: 32 filters, 8x8 kernel, stride 4\")\n",
    "print(\"Conv2: 64 filters, 4x4 kernel, stride 2\") \n",
    "print(\"Conv3: 64 filters, 3x3 kernel, stride 1\")\n",
    "print(f\"Flattened features: {info['conv_output_size']}\")\n",
    "print(\"FC1: 512 neurons\")\n",
    "print(f\"Output: {info['output_shape']} Q-values\")\n",
    "print(f\"\\nTotal parameters: {info['total_parameters']:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 4, 84, 84)\n",
    "with torch.no_grad():\n",
    "    output = dqn(dummy_input)\n",
    "    \n",
    "print(f\"\\nüß™ Example forward pass:\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output Q-values: {output.squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Architecture Analysis\n",
    "\n",
    "Our CNN processes **4 stacked frames** (84x84 each) to capture motion and temporal information:\n",
    "\n",
    "1. **Conv1**: Large receptive field (8x8) to detect basic features\n",
    "2. **Conv2**: Medium receptive field (4x4) for pattern combinations  \n",
    "3. **Conv3**: Small receptive field (3x3) for fine details\n",
    "4. **FC layers**: Combine features and output Q-values for each action\n",
    "\n",
    "The network outputs **Q-values** for each action, not action probabilities!\n",
    "\n",
    "---\n",
    "# Section 3: Experience Replay Buffer\n",
    "\n",
    "Experience replay is crucial for stable DQN training. Let's see why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer for storing and sampling transitions.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        \"\"\"\n",
    "        Initialize replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            capacity: Maximum number of transitions to store\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a transition to the buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            done: Whether episode ended\n",
    "        \"\"\"\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of transitions to sample\n",
    "            \n",
    "        Returns:\n",
    "            Batch of transitions as separate tensors\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            raise ValueError(f\"Buffer has only {len(self.buffer)} samples, need {batch_size}\")\n",
    "            \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unpack batch\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current buffer size.\"\"\"\n",
    "        return len(self.buffer)\n",
    "        \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get buffer statistics.\"\"\"\n",
    "        if len(self.buffer) == 0:\n",
    "            return {\"size\": 0, \"capacity\": self.capacity, \"utilization\": 0.0}\n",
    "            \n",
    "        rewards = [transition[2] for transition in self.buffer]\n",
    "        \n",
    "        stats = {\n",
    "            \"size\": len(self.buffer),\n",
    "            \"capacity\": self.capacity,\n",
    "            \"utilization\": len(self.buffer) / self.capacity,\n",
    "            \"avg_reward\": np.mean(rewards),\n",
    "            \"reward_std\": np.std(rewards),\n",
    "            \"min_reward\": np.min(rewards),\n",
    "            \"max_reward\": np.max(rewards)\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "# Demonstrate experience replay\n",
    "print(\"üóÉÔ∏è Experience Replay Buffer Demo\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create buffer\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "print(\"Adding sample experiences to buffer...\")\n",
    "\n",
    "# Add some dummy experiences\n",
    "for i in range(150):\n",
    "    state = np.random.random((4, 84, 84))\n",
    "    action = np.random.randint(0, 3)\n",
    "    reward = np.random.normal(0, 1)  # Random reward\n",
    "    next_state = np.random.random((4, 84, 84))\n",
    "    done = np.random.random() < 0.1  # 10% chance of episode end\n",
    "    \n",
    "    buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "# Show buffer statistics\n",
    "stats = buffer.get_statistics()\n",
    "print(f\"\\nüìä Buffer Statistics:\")\n",
    "print(f\"  Size: {stats['size']}/{stats['capacity']}\")\n",
    "print(f\"  Utilization: {stats['utilization']:.1%}\")\n",
    "print(f\"  Average reward: {stats['avg_reward']:.3f}\")\n",
    "print(f\"  Reward std: {stats['reward_std']:.3f}\")\n",
    "\n",
    "# Demonstrate sampling\n",
    "print(f\"\\nüé≤ Sampling batch of 32 experiences...\")\n",
    "states, actions, rewards, next_states, dones = buffer.sample(32)\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  States: {states.shape}\")\n",
    "print(f\"  Actions: {actions.shape}\")\n",
    "print(f\"  Rewards: {rewards.shape}\")\n",
    "print(f\"  Next states: {next_states.shape}\")\n",
    "print(f\"  Dones: {dones.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Benefits of Experience Replay:\")\n",
    "print(\"‚Ä¢ Breaks correlation between consecutive experiences\")\n",
    "print(\"‚Ä¢ Enables multiple learning updates from same experience\")\n",
    "print(\"‚Ä¢ Improves sample efficiency\")\n",
    "print(\"‚Ä¢ Stabilizes training by mixing old and new experiences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the importance of experience replay\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Simulate correlated vs uncorrelated data\n",
    "steps = np.arange(100)\n",
    "correlated_rewards = np.cumsum(np.random.randn(100) * 0.1) + np.sin(steps * 0.1) * 2\n",
    "uncorrelated_rewards = np.random.randn(100) * 2\n",
    "\n",
    "ax1.plot(steps, correlated_rewards, 'r-', label='Sequential (Correlated)', linewidth=2)\n",
    "ax1.set_title('Without Experience Replay')\n",
    "ax1.set_xlabel('Training Step')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.scatter(steps, uncorrelated_rewards, c='blue', alpha=0.6, label='Random Sampling')\n",
    "ax2.set_title('With Experience Replay')\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('Reward')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experience Replay Breaks Temporal Correlation', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° The left plot shows correlated sequential data that can bias learning.\")\n",
    "print(\"üí° The right plot shows how random sampling breaks these correlations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Target Network Mechanism\n",
    "\n",
    "Target networks prevent the \"moving target\" problem in Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetworkDemo:\n",
    "    \"\"\"Demonstrates the target network concept.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.main_network = DQNNetwork()\n",
    "        self.target_network = DQNNetwork()\n",
    "        \n",
    "        # Copy main network weights to target network\n",
    "        self.hard_update()\n",
    "        \n",
    "    def hard_update(self):\n",
    "        \"\"\"Copy main network weights to target network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        \n",
    "    def soft_update(self, tau: float = 0.001):\n",
    "        \"\"\"\n",
    "        Soft update target network weights.\n",
    "        \n",
    "        Args:\n",
    "            tau: Soft update parameter (0 = no update, 1 = hard update)\n",
    "        \"\"\"\n",
    "        for target_param, main_param in zip(\n",
    "            self.target_network.parameters(), \n",
    "            self.main_network.parameters()\n",
    "        ):\n",
    "            target_param.data.copy_(\n",
    "                tau * main_param.data + (1.0 - tau) * target_param.data\n",
    "            )\n",
    "            \n",
    "    def compare_networks(self) -> float:\n",
    "        \"\"\"Compare parameter differences between networks.\"\"\"\n",
    "        total_diff = 0.0\n",
    "        total_params = 0\n",
    "        \n",
    "        for target_param, main_param in zip(\n",
    "            self.target_network.parameters(),\n",
    "            self.main_network.parameters()\n",
    "        ):\n",
    "            diff = torch.norm(target_param - main_param).item()\n",
    "            total_diff += diff\n",
    "            total_params += target_param.numel()\n",
    "            \n",
    "        return total_diff / total_params\n",
    "\n",
    "# Demonstrate target network mechanism\n",
    "print(\"üéØ Target Network Demonstration\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "demo = TargetNetworkDemo()\n",
    "\n",
    "print(\"Target Network Concept:\")\n",
    "print(\"-\" * 25)\n",
    "print(\"Main Network:   Used for action selection and learning\")\n",
    "print(\"Target Network: Used for Q-target calculation (stable)\")\n",
    "print()\n",
    "\n",
    "# Show initial state\n",
    "initial_diff = demo.compare_networks()\n",
    "print(f\"Initial parameter difference: {initial_diff:.6f}\")\n",
    "\n",
    "# Simulate training updates to main network\n",
    "optimizer = optim.Adam(demo.main_network.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nSimulating training updates...\")\n",
    "differences = []\n",
    "steps = []\n",
    "\n",
    "for step in range(20):\n",
    "    # Dummy loss to update main network\n",
    "    dummy_input = torch.randn(1, 4, 84, 84)\n",
    "    loss = demo.main_network(dummy_input).sum()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Measure difference\n",
    "    diff = demo.compare_networks()\n",
    "    differences.append(diff)\n",
    "    steps.append(step)\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step:2d}: Parameter difference = {diff:.6f}\")\n",
    "        \n",
    "    # Hard update every 10 steps\n",
    "    if step == 10:\n",
    "        demo.hard_update()\n",
    "        print(f\"        üîÑ Hard update performed!\")\n",
    "        \n",
    "print(\"\\n‚úÖ Target Network Benefits:\")\n",
    "print(\"‚Ä¢ Prevents moving target problem in Q-learning\")\n",
    "print(\"‚Ä¢ Stabilizes training by providing consistent targets\")\n",
    "print(\"‚Ä¢ Reduces correlation between Q-values and targets\")\n",
    "print(\"‚Ä¢ Hard updates every N steps maintain stability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target network updates\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(steps, differences, 'b-o', linewidth=2, markersize=6)\n",
    "plt.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='Hard Update')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Parameter Difference')\n",
    "plt.title('Target Network Parameter Divergence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Notice how the parameter difference grows as the main network learns,\")\n",
    "print(\"üí° then resets to zero when we perform a hard update at step 10.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Target Network Math\n",
    "\n",
    "**Without Target Network** (unstable):\n",
    "```\n",
    "Target = r + Œ≥ * max Q_Œ∏(s', a')\n",
    "Loss = (Q_Œ∏(s,a) - Target)¬≤\n",
    "```\n",
    "\n",
    "**With Target Network** (stable):\n",
    "```\n",
    "Target = r + Œ≥ * max Q_Œ∏'(s', a')  ‚Üê Œ∏' is fixed!\n",
    "Loss = (Q_Œ∏(s,a) - Target)¬≤\n",
    "```\n",
    "\n",
    "The target network Œ∏' is updated every few thousand steps, providing stable targets.\n",
    "\n",
    "---\n",
    "# Section 5: Epsilon-Greedy Strategy\n",
    "\n",
    "Balancing exploration vs exploitation is crucial for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy:\n",
    "    \"\"\"Implements epsilon-greedy exploration strategy.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon_start: float = 1.0, epsilon_end: float = 0.01, \n",
    "                 epsilon_decay: float = 0.995):\n",
    "        \"\"\"\n",
    "        Initialize epsilon-greedy strategy.\n",
    "        \n",
    "        Args:\n",
    "            epsilon_start: Initial exploration rate\n",
    "            epsilon_end: Final exploration rate\n",
    "            epsilon_decay: Decay factor per episode\n",
    "        \"\"\"\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon = epsilon_start\n",
    "        self.episode = 0\n",
    "        \n",
    "    def get_action(self, q_values: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Select action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            q_values: Q-values for all actions\n",
    "            \n",
    "        Returns:\n",
    "            Selected action index\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.randint(len(q_values))\n",
    "        else:\n",
    "            # Exploit: best action\n",
    "            return q_values.argmax().item()\n",
    "            \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Update epsilon for next episode.\"\"\"\n",
    "        self.epsilon = max(\n",
    "            self.epsilon_end,\n",
    "            self.epsilon * self.epsilon_decay\n",
    "        )\n",
    "        self.episode += 1\n",
    "        \n",
    "    def get_epsilon_schedule(self, num_episodes: int) -> List[float]:\n",
    "        \"\"\"Get epsilon values for given number of episodes.\"\"\"\n",
    "        epsilons = []\n",
    "        epsilon = self.epsilon_start\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            epsilons.append(epsilon)\n",
    "            epsilon = max(self.epsilon_end, epsilon * self.epsilon_decay)\n",
    "            \n",
    "        return epsilons\n",
    "\n",
    "# Demonstrate epsilon-greedy strategy\n",
    "print(\"üé≤ Epsilon-Greedy Strategy Demo\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "strategy = EpsilonGreedyStrategy(\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995\n",
    ")\n",
    "\n",
    "# Get epsilon schedule\n",
    "num_episodes = 500\n",
    "epsilon_schedule = strategy.get_epsilon_schedule(num_episodes)\n",
    "\n",
    "# Show key points\n",
    "print(\"Epsilon Decay Schedule:\")\n",
    "print(\"-\" * 22)\n",
    "milestones = [0, 50, 100, 200, 300, 400, 499]\n",
    "for ep in milestones:\n",
    "    print(f\"Episode {ep:3d}: Œµ = {epsilon_schedule[ep]:.4f}\")\n",
    "    \n",
    "# Simulate action selection\n",
    "print(f\"\\nüéØ Action Selection Simulation (Episode 0, Œµ = {epsilon_schedule[0]:.3f}):\")\n",
    "dummy_q_values = torch.tensor([0.1, 0.8, 0.3])  # [steering, gas, brake]\n",
    "\n",
    "actions = []\n",
    "for _ in range(100):\n",
    "    action = strategy.get_action(dummy_q_values)\n",
    "    actions.append(action)\n",
    "    \n",
    "action_counts = np.bincount(actions, minlength=3)\n",
    "action_names = ['Steering', 'Gas', 'Brake']\n",
    "\n",
    "print(\"Action distribution (100 selections):\")\n",
    "for i, (name, count) in enumerate(zip(action_names, action_counts)):\n",
    "    percentage = count / 100 * 100\n",
    "    q_val = dummy_q_values[i].item()\n",
    "    print(f\"  {name:8}: {count:2d}/100 ({percentage:4.1f}%) [Q={q_val:.1f}]\")\n",
    "    \n",
    "print(f\"\\nüí° Gas has highest Q-value ({dummy_q_values[1]:.1f}) but random\")\n",
    "print(\"üí° exploration still selects other actions frequently.\")\n",
    "\n",
    "print(\"\\n‚úÖ Exploration vs Exploitation Trade-off:\")\n",
    "print(\"‚Ä¢ High Œµ: More exploration, discovers new strategies\")\n",
    "print(\"‚Ä¢ Low Œµ:  More exploitation, uses learned knowledge\")\n",
    "print(\"‚Ä¢ Decay:  Gradually shift from exploration to exploitation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize epsilon decay schedule\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Epsilon decay over episodes\n",
    "episodes = range(len(epsilon_schedule))\n",
    "ax1.plot(episodes, epsilon_schedule, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Epsilon (Œµ)')\n",
    "ax1.set_title('Epsilon Decay Schedule')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Œµ = 0.1')\n",
    "ax1.legend()\n",
    "\n",
    "# Exploration vs Exploitation over time\n",
    "exploration_rate = np.array(epsilon_schedule)\n",
    "exploitation_rate = 1 - exploration_rate\n",
    "\n",
    "ax2.fill_between(episodes, 0, exploration_rate, alpha=0.6, color='red', label='Exploration')\n",
    "ax2.fill_between(episodes, exploration_rate, 1, alpha=0.6, color='blue', label='Exploitation')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.set_title('Exploration vs Exploitation Balance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Early episodes: High exploration to discover the environment\")\n",
    "print(\"üí° Later episodes: High exploitation to use learned knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: Loss Function and Optimization\n",
    "\n",
    "Let's see how DQN actually learns from experience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_loss_function():\n",
    "    \"\"\"Demonstrate DQN loss function and optimization.\"\"\"\n",
    "    print(\"üìä DQN Loss Function Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"DQN Loss Function (Temporal Difference Error):\")\n",
    "    print(\"-\" * 48)\n",
    "    print(\"Target: y = r + Œ≥ * max(Q_target(s', a'))\")\n",
    "    print(\"Loss:   L = Huber(Q_main(s, a) - y)\")\n",
    "    print()\n",
    "    \n",
    "    # Create networks\n",
    "    main_network = DQNNetwork()\n",
    "    target_network = DQNNetwork()\n",
    "    target_network.load_state_dict(main_network.state_dict())\n",
    "    \n",
    "    # Dummy batch\n",
    "    batch_size = 4\n",
    "    states = torch.randn(batch_size, 4, 84, 84)\n",
    "    actions = torch.LongTensor([0, 1, 2, 1])  # [left, straight, right, straight]\n",
    "    rewards = torch.FloatTensor([0.1, 1.0, -0.5, 0.8])\n",
    "    next_states = torch.randn(batch_size, 4, 84, 84)\n",
    "    dones = torch.BoolTensor([False, False, True, False])\n",
    "    gamma = 0.99\n",
    "    \n",
    "    print(\"üì¶ Example Batch:\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Actions: {actions.tolist()}\")\n",
    "    print(f\"  Rewards: {rewards.tolist()}\")\n",
    "    print(f\"  Dones: {dones.tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        # Current Q-values\n",
    "        current_q_values = main_network(states)\n",
    "        current_q_values_selected = current_q_values.gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Next Q-values from target network\n",
    "        next_q_values = target_network(next_states)\n",
    "        next_q_values_max = next_q_values.max(1)[0]\n",
    "        \n",
    "        # Compute targets\n",
    "        targets = rewards + (gamma * next_q_values_max * (~dones))\n",
    "        \n",
    "    print(\"üßÆ Q-Value Computation:\")\n",
    "    print(f\"  Current Q-values shape: {current_q_values.shape}\")\n",
    "    print(f\"  Selected Q-values: {current_q_values_selected.squeeze().detach().numpy()}\")\n",
    "    print(f\"  Next max Q-values: {next_q_values_max.detach().numpy()}\")\n",
    "    print(f\"  Targets: {targets.detach().numpy()}\")\n",
    "    \n",
    "    # Compute loss\n",
    "    td_errors = current_q_values_selected.squeeze() - targets\n",
    "    huber_loss = F.smooth_l1_loss(current_q_values_selected.squeeze(), targets)\n",
    "    mse_loss = F.mse_loss(current_q_values_selected.squeeze(), targets)\n",
    "    \n",
    "    print(f\"\\nüìâ Loss Computation:\")\n",
    "    print(f\"  TD errors: {td_errors.detach().numpy()}\")\n",
    "    print(f\"  Huber loss: {huber_loss.item():.4f}\")\n",
    "    print(f\"  MSE loss: {mse_loss.item():.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Why Huber Loss?\")\n",
    "    print(\"‚Ä¢ Less sensitive to outliers than MSE\")\n",
    "    print(\"‚Ä¢ Provides stable gradients for large errors\")\n",
    "    print(\"‚Ä¢ Behaves like MSE for small errors, MAE for large errors\")\n",
    "    print(\"‚Ä¢ Improves training stability\")\n",
    "    \n",
    "    return td_errors.detach().numpy(), huber_loss.item(), mse_loss.item()\n",
    "\n",
    "# Run the demonstration\n",
    "td_errors, huber_loss, mse_loss = demonstrate_loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different loss functions\n",
    "errors = np.linspace(-3, 3, 100)\n",
    "mse_losses = errors ** 2\n",
    "mae_losses = np.abs(errors)\n",
    "huber_losses = np.where(np.abs(errors) <= 1, 0.5 * errors**2, np.abs(errors) - 0.5)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(errors, mse_losses, 'r-', label='MSE Loss', linewidth=2)\n",
    "plt.plot(errors, mae_losses, 'g-', label='MAE Loss', linewidth=2)\n",
    "plt.plot(errors, huber_losses, 'b-', label='Huber Loss', linewidth=2)\n",
    "plt.xlabel('TD Error')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Comparison of Loss Functions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Huber loss combines the best of both worlds:\")\n",
    "print(\"üí° ‚Ä¢ Quadratic for small errors (like MSE)\")\n",
    "print(\"üí° ‚Ä¢ Linear for large errors (like MAE)\")\n",
    "print(\"üí° This prevents large outliers from dominating the gradient updates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Training Algorithm Summary\n",
    "\n",
    "Here's the complete DQN training algorithm:\n",
    "\n",
    "```python\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 1. Select action using Œµ-greedy\n",
    "        if random() < epsilon:\n",
    "            action = random_action()\n",
    "        else:\n",
    "            action = argmax(Q_main(state))\n",
    "        \n",
    "        # 2. Take action and observe\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # 3. Store in replay buffer\n",
    "        buffer.store(state, action, reward, next_state, done)\n",
    "        \n",
    "        # 4. Sample batch and train\n",
    "        if len(buffer) > batch_size:\n",
    "            batch = buffer.sample(batch_size)\n",
    "            \n",
    "            # Compute targets using target network\n",
    "            targets = rewards + Œ≥ * max(Q_target(next_states))\n",
    "            \n",
    "            # Compute loss and update main network\n",
    "            loss = huber_loss(Q_main(states, actions), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 5. Update target network periodically\n",
    "        if step % target_update_freq == 0:\n",
    "            Q_target = copy(Q_main)\n",
    "        \n",
    "        state = next_state\n",
    "        if done: break\n",
    "    \n",
    "    # 6. Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * decay)\n",
    "```\n",
    "\n",
    "---\n",
    "# üéâ Tutorial Complete!\n",
    "\n",
    "You now understand all the key components of DQN:\n",
    "\n",
    "‚úÖ **Neural networks** for Q-function approximation  \n",
    "‚úÖ **Experience replay** for stable learning  \n",
    "‚úÖ **Target networks** for training stability  \n",
    "‚úÖ **Epsilon-greedy** for exploration  \n",
    "‚úÖ **Huber loss** for robust optimization  \n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Run the training script**: `python training/dqn_training.py`\n",
    "2. **Test manual gameplay**: `python games/test_manual_play.py`\n",
    "3. **Demo trained agent**: `python games/demo_trained_agent.py`\n",
    "\n",
    "## üìö Further Reading\n",
    "\n",
    "- [Original DQN Paper](https://arxiv.org/abs/1312.5602)\n",
    "- [Human-level control through deep RL](https://www.nature.com/articles/nature14236)\n",
    "- [Rainbow DQN](https://arxiv.org/abs/1710.02298)\n",
    "\n",
    "Happy learning! üèéÔ∏èüí®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Network architecture\n",
    "layers = ['Input\\n(4√ó84√ó84)', 'Conv1\\n(32 filters)', 'Conv2\\n(64 filters)', 'Conv3\\n(64 filters)', 'FC1\\n(512)', 'Output\\n(3 Q-values)']\n",
    "ax1.barh(range(len(layers)), [1, 0.8, 0.6, 0.4, 0.3, 0.1], color='skyblue')\n",
    "ax1.set_yticks(range(len(layers)))\n",
    "ax1.set_yticklabels(layers)\n",
    "ax1.set_title('DQN Architecture')\n",
    "ax1.set_xlabel('Relative Size')\n",
    "\n",
    "# 2. Epsilon decay\n",
    "episodes = np.arange(500)\n",
    "epsilon = np.maximum(0.01, 1.0 * (0.995 ** episodes))\n",
    "ax2.plot(episodes, epsilon, 'b-', linewidth=2)\n",
    "ax2.set_title('Epsilon Decay')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Epsilon')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss function comparison\n",
    "x = np.linspace(-2, 2, 100)\n",
    "huber = np.where(np.abs(x) <= 1, 0.5 * x**2, np.abs(x) - 0.5)\n",
    "mse = x**2\n",
    "ax3.plot(x, mse, 'r--', label='MSE', linewidth=2)\n",
    "ax3.plot(x, huber, 'b-', label='Huber', linewidth=2)\n",
    "ax3.set_title('Loss Functions')\n",
    "ax3.set_xlabel('TD Error')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. DQN components\n",
    "components = ['Neural\\nNetwork', 'Experience\\nReplay', 'Target\\nNetwork', 'Œµ-Greedy', 'Huber\\nLoss']\n",
    "importance = [0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "ax4.pie(importance, labels=components, colors=colors, autopct='%1.0f%%', startangle=90)\n",
    "ax4.set_title('DQN Components')\n",
    "\n",
    "plt.suptitle('DQN Tutorial Summary', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéì Congratulations! You've completed the DQN tutorial.\")\n",
    "print(\"üèÅ Ready to train your own racing agent!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}