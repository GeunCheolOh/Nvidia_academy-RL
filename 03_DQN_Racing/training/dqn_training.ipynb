{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarRacing í™˜ê²½ì„ ìœ„í•œ DQN í•™ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ CarRacing-v3 í™˜ê²½ì„ ìœ„í•œ ì™„ì „í•œ DQN í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤. Deep Q-Networksë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ì°¨ë¥¼ ìš´ì „í•˜ëŠ” AI ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤!\n",
    "\n",
    "## ìˆ˜í–‰í•  ì‘ì—…\n",
    "1. **í™˜ê²½ ì„¤ì •** - ì „ì²˜ë¦¬ê°€ í¬í•¨ëœ CarRacing\n",
    "2. **DQN ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•** - ì‹œê°ì  ì…ë ¥ ì²˜ë¦¬ë¥¼ ìœ„í•œ CNN\n",
    "3. **í•™ìŠµ êµ¬ì„±ìš”ì†Œ êµ¬í˜„** - Replay buffer, target network ë“±\n",
    "4. **ì—ì´ì „íŠ¸ í•™ìŠµ** - ìš´ì „ì„ í•™ìŠµí•˜ëŠ” ê³¼ì • ê´€ì°°!\n",
    "5. **ê²°ê³¼ ë¶„ì„** - í•™ìŠµ ì§„í–‰ ìƒí™© ì‹œê°í™”\n",
    "\n",
    "## ë¹ ë¥¸ ì‹œì‘\n",
    "ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\n",
    "- ê°€ìƒ í™˜ê²½ í™œì„±í™”\n",
    "- ëª¨ë“  ì¢…ì†ì„± ì„¤ì¹˜\n",
    "- íŠœí† ë¦¬ì–¼ ë…¸íŠ¸ë¶ì„ ë¨¼ì € ì‹¤í–‰ (ê¶Œì¥)\n",
    "\n",
    "ë ˆì´ì‹± AI í•™ìŠµì„ ì‹œì‘í•©ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import Tuple, List, Optional, Dict, Any\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# ì¥ì¹˜ í™•ì¸\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
    "print(f\"ğŸ Python íŒ¨í‚¤ì§€ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "\n",
    "# ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ ì„¤ì •\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "print(f\"ğŸ² ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "\n",
    "ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì€ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œì–´í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê°’ìœ¼ë¡œ ì‹¤í—˜í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° - ì‹¤í—˜ì„ ìœ„í•´ ì´ ê°’ë“¤ì„ ìˆ˜ì •í•´ë³´ì„¸ìš”!\n",
    "HYPERPARAMETERS = {\n",
    "    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "    'learning_rate': 0.0001,\n",
    "    'gamma': 0.99,  # í• ì¸ ì¸ì\n",
    "    \n",
    "    # íƒí—˜ íŒŒë¼ë¯¸í„°\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    \n",
    "    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "    'batch_size': 32,\n",
    "    'buffer_size': 10000,\n",
    "    'target_update': 1000,  # N ìŠ¤í…ë§ˆë‹¤ target network ì—…ë°ì´íŠ¸\n",
    "    \n",
    "    # ì—í”¼ì†Œë“œ íŒŒë¼ë¯¸í„°\n",
    "    'num_episodes': 100,  # ë…¸íŠ¸ë¶ìš©ìœ¼ë¡œ ì ì€ ì—í”¼ì†Œë“œë¡œ ì‹œì‘\n",
    "    'max_steps_per_episode': 1000,\n",
    "    \n",
    "    # í™˜ê²½ íŒŒë¼ë¯¸í„°\n",
    "    'frame_stack': 4,\n",
    "    'image_size': (84, 84),\n",
    "    \n",
    "    # ë¡œê¹…\n",
    "    'save_interval': 25,\n",
    "    'log_interval': 5\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "for key, value in HYPERPARAMETERS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nğŸ’¡ íŒ: ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ 'num_episodes'ë¥¼ 500+ë¡œ ì¦ê°€ì‹œí‚¤ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## í™˜ê²½ ì„¤ì •\n",
    "\n",
    "ì ì ˆí•œ ì „ì²˜ë¦¬ì™€ í•¨ê»˜ CarRacing í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRacingWrapper:\n",
    "    \"\"\"ì „ì²˜ë¦¬ê°€ í¬í•¨ëœ CarRacing í™˜ê²½ ë˜í¼\"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        CarRacing í™˜ê²½ ë˜í¼ ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            render_mode: ë Œë”ë§ ëª¨ë“œ ('human', 'rgb_array', ë˜ëŠ” None)\n",
    "        \"\"\"\n",
    "        self.env = gym.make('CarRacing-v3', render_mode=render_mode)\n",
    "        self.frame_stack = HYPERPARAMETERS['frame_stack']\n",
    "        self.image_size = HYPERPARAMETERS['image_size']\n",
    "        \n",
    "        # í”„ë ˆì„ ìŠ¤íƒœí‚¹ì„ ìœ„í•œ ë²„í¼\n",
    "        self.frames = deque(maxlen=self.frame_stack)\n",
    "        \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"í™˜ê²½ ë¦¬ì…‹ ë° ì´ˆê¸° ìŠ¤íƒ í”„ë ˆì„ ë°˜í™˜\"\"\"\n",
    "        obs, info = self.env.reset()\n",
    "        \n",
    "        # ì´ˆê¸° í”„ë ˆì„ ì „ì²˜ë¦¬\n",
    "        processed_frame = self._preprocess_frame(obs)\n",
    "        \n",
    "        # ì²« í”„ë ˆì„ì„ ë°˜ë³µí•˜ì—¬ í”„ë ˆì„ ìŠ¤íƒ ì´ˆê¸°í™”\n",
    "        for _ in range(self.frame_stack):\n",
    "            self.frames.append(processed_frame)\n",
    "            \n",
    "        return self._get_stacked_frames()\n",
    "        \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"\n",
    "        í–‰ë™ì„ ìˆ˜í–‰í•˜ê³  ì „ì²˜ë¦¬ëœ ê´€ì¸¡ê°’ ë°˜í™˜\n",
    "        \n",
    "        Args:\n",
    "            action: ì´ì‚° í–‰ë™ ì¸ë±ìŠ¤\n",
    "            \n",
    "        Returns:\n",
    "            (ê´€ì¸¡ê°’, ë³´ìƒ, ì¢…ë£Œì—¬ë¶€, ì˜ë¦¼ì—¬ë¶€, ì •ë³´) íŠœí”Œ\n",
    "        \"\"\"\n",
    "        # ì´ì‚° í–‰ë™ì„ ì—°ì† í–‰ë™ìœ¼ë¡œ ë³€í™˜\n",
    "        continuous_action = self._discrete_to_continuous(action)\n",
    "        \n",
    "        # í™˜ê²½ì—ì„œ ìŠ¤í… ìˆ˜í–‰\n",
    "        obs, reward, terminated, truncated, info = self.env.step(continuous_action)\n",
    "        \n",
    "        # í”„ë ˆì„ ì „ì²˜ë¦¬ ë° ìŠ¤íƒ\n",
    "        processed_frame = self._preprocess_frame(obs)\n",
    "        self.frames.append(processed_frame)\n",
    "        stacked_frames = self._get_stacked_frames()\n",
    "        \n",
    "        return stacked_frames, reward, terminated, truncated, info\n",
    "        \n",
    "    def _preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"í”„ë ˆì„ ì „ì²˜ë¦¬: í¬ê¸° ì¡°ì •, ê·¸ë ˆì´ìŠ¤ì¼€ì¼, ì •ê·œí™”\"\"\"\n",
    "        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # íƒ€ê²Ÿ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "        resized_frame = cv2.resize(gray_frame, self.image_size)\n",
    "        \n",
    "        # [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”\n",
    "        normalized_frame = resized_frame.astype(np.float32) / 255.0\n",
    "        \n",
    "        return normalized_frame\n",
    "        \n",
    "    def _get_stacked_frames(self) -> np.ndarray:\n",
    "        \"\"\"ìŠ¤íƒëœ í”„ë ˆì„ì„ numpy ë°°ì—´ë¡œ ë°˜í™˜\"\"\"\n",
    "        return np.array(list(self.frames))\n",
    "        \n",
    "    def _discrete_to_continuous(self, action: int) -> np.ndarray:\n",
    "        \"\"\"ì´ì‚° í–‰ë™ì„ ì—°ì† í–‰ë™ ê³µê°„ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        if action == 0:     # ì™¼ìª½ íšŒì „\n",
    "            return np.array([-0.5, 0.3, 0.0])\n",
    "        elif action == 1:   # ì§ì§„\n",
    "            return np.array([0.0, 0.5, 0.0])\n",
    "        elif action == 2:   # ì˜¤ë¥¸ìª½ íšŒì „\n",
    "            return np.array([0.5, 0.3, 0.0])\n",
    "        elif action == 3:   # ë¸Œë ˆì´í¬\n",
    "            return np.array([0.0, 0.0, 0.8])\n",
    "        else:\n",
    "            return np.array([0.0, 0.0, 0.0])\n",
    "            \n",
    "    def close(self):\n",
    "        \"\"\"í™˜ê²½ ì¢…ë£Œ\"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "# í™˜ê²½ í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸš— CarRacing í™˜ê²½ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "test_env = CarRacingWrapper()\n",
    "test_obs = test_env.reset()\n",
    "print(f\"âœ… í™˜ê²½ ì´ˆê¸°í™” ì„±ê³µ!\")\n",
    "print(f\"   ê´€ì¸¡ê°’ í˜•íƒœ: {test_obs.shape}\")\n",
    "print(f\"   í–‰ë™ ê³µê°„: 4ê°œ ì´ì‚° í–‰ë™ (ì™¼ìª½, ì§ì§„, ì˜¤ë¥¸ìª½, ë¸Œë ˆì´í¬)\")\n",
    "test_env.close()\n",
    "del test_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DQN ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°\n",
    "\n",
    "CNN ê¸°ë°˜ Deep Q-Networkë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"CarRacingì„ ìœ„í•œ CNN ê¸°ë°˜ Deep Q-Network\"\"\"\n",
    "    \n",
    "    def __init__(self, action_dim: int = 4, input_channels: int = 4):\n",
    "        \"\"\"\n",
    "        DQN ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            action_dim: ì´ì‚° í–‰ë™ì˜ ê°œìˆ˜\n",
    "            input_channels: ì…ë ¥ ì±„ë„ ìˆ˜ (í”„ë ˆì„ ìŠ¤íƒ)\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # í•©ì„±ê³± ë ˆì´ì–´\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        # Conv ì¶œë ¥ í¬ê¸° ê³„ì‚°\n",
    "        self._conv_output_size = self._get_conv_output_size((input_channels, 84, 84))\n",
    "        \n",
    "        # ì™„ì „ ì—°ê²° ë ˆì´ì–´\n",
    "        self.fc1 = nn.Linear(self._conv_output_size, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _get_conv_output_size(self, input_shape: Tuple[int, int, int]) -> int:\n",
    "        \"\"\"Conv ë ˆì´ì–´ í†µê³¼ í›„ ì¶œë ¥ í¬ê¸° ê³„ì‚°\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            dummy_output = self._forward_conv(dummy_input)\n",
    "            return dummy_output.numel()\n",
    "            \n",
    "    def _forward_conv(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Conv ë ˆì´ì–´ë§Œ í†µê³¼í•˜ëŠ” forward pass\"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return x.view(x.size(0), -1)\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "                    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"ë„¤íŠ¸ì›Œí¬ forward pass\"\"\"\n",
    "        x = self._forward_conv(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ë„¤íŠ¸ì›Œí¬ ìƒì„± ë° ë¶„ì„\n",
    "dqn = DQN(action_dim=4, input_channels=4).to(device)\n",
    "total_params = sum(p.numel() for p in dqn.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"ğŸ§  DQN ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"   ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
    "print(f\"   ë„¤íŠ¸ì›Œí¬ í¬ê¸°: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Forward pass í…ŒìŠ¤íŠ¸\n",
    "dummy_input = torch.randn(1, 4, 84, 84).to(device)\n",
    "with torch.no_grad():\n",
    "    output = dqn(dummy_input)\n",
    "print(f\"   ì…ë ¥ í˜•íƒœ: {dummy_input.shape}\")\n",
    "print(f\"   ì¶œë ¥ í˜•íƒœ: {output.shape}\")\n",
    "print(f\"âœ… ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experience Replay Buffer\n",
    "\n",
    "Replay bufferëŠ” í•™ìŠµì„ ìœ„í•œ ê²½í—˜ì„ ì €ì¥í•˜ê³  ìƒ˜í”Œë§í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"ì „í™˜ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ê²½í—˜ ì¬ìƒ ë²„í¼\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"ë²„í¼ì— ì „í™˜ ì¶”ê°€\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        \"\"\"ì „í™˜ ë°°ì¹˜ ìƒ˜í”Œë§\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.BoolTensor(dones)\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Replay buffer ìƒì„±\n",
    "replay_buffer = ReplayBuffer(HYPERPARAMETERS['buffer_size'])\n",
    "print(f\"ğŸ—ƒï¸  Replay buffer ìƒì„± ì™„ë£Œ (ìš©ëŸ‰: {HYPERPARAMETERS['buffer_size']:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DQN ì—ì´ì „íŠ¸\n",
    "\n",
    "ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ ê²°í•©í•œ DQN ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"ëª¨ë“  í•™ìŠµ êµ¬ì„±ìš”ì†Œë¥¼ í¬í•¨í•œ DQN ì—ì´ì „íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device = device\n",
    "        self.action_dim = 4  # ì™¼ìª½, ì§ì§„, ì˜¤ë¥¸ìª½, ë¸Œë ˆì´í¬\n",
    "        \n",
    "        # ë„¤íŠ¸ì›Œí¬\n",
    "        self.main_network = DQN(self.action_dim).to(device)\n",
    "        self.target_network = DQN(self.action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì €\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.main_network.parameters(), \n",
    "            lr=HYPERPARAMETERS['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(HYPERPARAMETERS['buffer_size'])\n",
    "        \n",
    "        # íƒí—˜ ì „ëµ\n",
    "        self.epsilon = HYPERPARAMETERS['epsilon_start']\n",
    "        self.epsilon_decay = HYPERPARAMETERS['epsilon_decay']\n",
    "        self.epsilon_min = HYPERPARAMETERS['epsilon_end']\n",
    "        \n",
    "        # í•™ìŠµ ì¹´ìš´í„°\n",
    "        self.step_count = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "    def select_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"Epsilon-greedy ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.main_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "            \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Replay bufferì— ì „í™˜ ì €ì¥\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def update(self) -> Optional[float]:\n",
    "        \"\"\"Replay bufferì˜ ë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        if len(self.replay_buffer) < HYPERPARAMETERS['batch_size']:\n",
    "            return None\n",
    "            \n",
    "        # ë°°ì¹˜ ìƒ˜í”Œë§\n",
    "        states, actions, rewards, next_states, dones = \\\n",
    "            self.replay_buffer.sample(HYPERPARAMETERS['batch_size'])\n",
    "            \n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # í˜„ì¬ Q-ê°’\n",
    "        current_q_values = self.main_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target networkì˜ ë‹¤ìŒ Q-ê°’\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            targets = rewards + (HYPERPARAMETERS['gamma'] * next_q_values * (~dones))\n",
    "            \n",
    "        # ì†ì‹¤ ê³„ì‚°\n",
    "        loss = F.smooth_l1_loss(current_q_values.squeeze(), targets)\n",
    "        \n",
    "        # ìµœì í™”\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.main_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # ìŠ¤í… ì¹´ìš´í„° ì—…ë°ì´íŠ¸\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Target network ì—…ë°ì´íŠ¸\n",
    "        if self.step_count % HYPERPARAMETERS['target_update'] == 0:\n",
    "            self.update_target_network()\n",
    "            \n",
    "        return loss.item()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Main network ê°€ì¤‘ì¹˜ë¡œ target network ì—…ë°ì´íŠ¸\"\"\"\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"ë‹¤ìŒ ì—í”¼ì†Œë“œë¥¼ ìœ„í•œ epsilon ì—…ë°ì´íŠ¸\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.episode_count += 1\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ìƒì„±\n",
    "agent = DQNAgent(device)\n",
    "print(f\"ğŸ¤– DQN ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"   Main network íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in agent.main_network.parameters()):,}\")\n",
    "print(f\"   Target network íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in agent.target_network.parameters()):,}\")\n",
    "print(f\"   ì´ˆê¸° epsilon: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## í•™ìŠµ ë£¨í”„\n",
    "\n",
    "ì´ì œ ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤! ì—¬ê¸°ì„œ ë§ˆë²•ì´ ì¼ì–´ë‚©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(num_episodes: int = HYPERPARAMETERS['num_episodes']):\n",
    "    \"\"\"DQN ì—ì´ì „íŠ¸ í•™ìŠµ\"\"\"\n",
    "    \n",
    "    # í™˜ê²½ ì´ˆê¸°í™”\n",
    "    env = CarRacingWrapper()\n",
    "    \n",
    "    # í•™ìŠµ í†µê³„\n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    # ì €ì¥ìš© ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "    models_dir = Path(\"../models/saved_weights\")\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"ğŸš€ {num_episodes} ì—í”¼ì†Œë“œ í•™ìŠµ ì‹œì‘...\")\n",
    "    print(f\"ğŸ“Š {HYPERPARAMETERS['log_interval']} ì—í”¼ì†Œë“œë§ˆë‹¤ ì§„í–‰ìƒí™© í‘œì‹œ\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    # ì§„í–‰ë°”ì™€ í•¨ê»˜ í•™ìŠµ ë£¨í”„\n",
    "    progress_bar = tqdm(range(num_episodes), desc=\"í•™ìŠµ ì¤‘\")\n",
    "    \n",
    "    for episode in progress_bar:\n",
    "        # í™˜ê²½ ë¦¬ì…‹\n",
    "        state = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        episode_loss_list = []\n",
    "        step = 0\n",
    "        \n",
    "        # ì—í”¼ì†Œë“œ ë£¨í”„\n",
    "        for step in range(HYPERPARAMETERS['max_steps_per_episode']):\n",
    "            # í–‰ë™ ì„ íƒ ë° ìˆ˜í–‰\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            # ì „í™˜ ì €ì¥\n",
    "            done = terminated or truncated\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_loss_list.append(loss)\n",
    "                \n",
    "            # ìƒíƒœ ë° ë³´ìƒ ì—…ë°ì´íŠ¸\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # í†µê³„ ì—…ë°ì´íŠ¸\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        avg_loss = np.mean(episode_loss_list) if episode_loss_list else 0.0\n",
    "        episode_losses.append(avg_loss)\n",
    "        \n",
    "        # íƒí—˜ ì—…ë°ì´íŠ¸\n",
    "        agent.update_epsilon()\n",
    "        \n",
    "        # ì§„í–‰ë°” ì—…ë°ì´íŠ¸\n",
    "        recent_rewards = episode_rewards[-10:] if len(episode_rewards) >= 10 else episode_rewards\n",
    "        avg_reward = np.mean(recent_rewards)\n",
    "        progress_bar.set_postfix({\n",
    "            'ë³´ìƒ': f'{episode_reward:.1f}',\n",
    "            'í‰ê· ': f'{avg_reward:.1f}',\n",
    "            'Îµ': f'{agent.epsilon:.3f}'\n",
    "        })\n",
    "        \n",
    "        # ë¡œê¹…\n",
    "        if episode % HYPERPARAMETERS['log_interval'] == 0 and episode > 0:\n",
    "            print(f\"\\nì—í”¼ì†Œë“œ {episode:4d} | \"\n",
    "                  f\"ë³´ìƒ: {episode_reward:8.2f} | \"\n",
    "                  f\"í‰ê·  ë³´ìƒ: {avg_reward:8.2f} | \"\n",
    "                  f\"ì†ì‹¤: {avg_loss:.4f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.4f} | \"\n",
    "                  f\"ë²„í¼: {len(agent.replay_buffer)}\")\n",
    "                  \n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        if episode % HYPERPARAMETERS['save_interval'] == 0 and episode > 0:\n",
    "            model_path = models_dir / f\"dqn_episode_{episode}.pth\"\n",
    "            torch.save({\n",
    "                'main_network': agent.main_network.state_dict(),\n",
    "                'target_network': agent.target_network.state_dict(),\n",
    "                'optimizer': agent.optimizer.state_dict(),\n",
    "                'epsilon': agent.epsilon,\n",
    "                'step_count': agent.step_count,\n",
    "                'episode_count': agent.episode_count\n",
    "            }, model_path)\n",
    "            \n",
    "            # ìµœê³  ëª¨ë¸ ì €ì¥\n",
    "            if episode_reward > best_reward:\n",
    "                best_reward = episode_reward\n",
    "                best_model_path = models_dir / \"dqn_best.pth\"\n",
    "                torch.save({\n",
    "                    'main_network': agent.main_network.state_dict(),\n",
    "                    'target_network': agent.target_network.state_dict(),\n",
    "                    'optimizer': agent.optimizer.state_dict(),\n",
    "                    'epsilon': agent.epsilon,\n",
    "                    'step_count': agent.step_count,\n",
    "                    'episode_count': agent.episode_count\n",
    "                }, best_model_path)\n",
    "                print(f\"ğŸ’¾ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥! ë³´ìƒ: {best_reward:.2f}\")\n",
    "    \n",
    "    # ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "    final_model_path = models_dir / \"dqn_final.pth\"\n",
    "    torch.save({\n",
    "        'main_network': agent.main_network.state_dict(),\n",
    "        'target_network': agent.target_network.state_dict(),\n",
    "        'optimizer': agent.optimizer.state_dict(),\n",
    "        'epsilon': agent.epsilon,\n",
    "        'step_count': agent.step_count,\n",
    "        'episode_count': agent.episode_count\n",
    "    }, final_model_path)\n",
    "    \n",
    "    # í•™ìŠµ ìš”ì•½\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ì´ ì—í”¼ì†Œë“œ: {len(episode_rewards)}\")\n",
    "    print(f\"ì´ ì‹œê°„: {total_time/60:.1f} ë¶„\")\n",
    "    print(f\"í‰ê·  ë³´ìƒ: {np.mean(episode_rewards):.2f}\")\n",
    "    print(f\"ìµœê³  ë³´ìƒ: {np.max(episode_rewards):.2f}\")\n",
    "    print(f\"ìµœì¢… epsilon: {agent.epsilon:.4f}\")\n",
    "    print(f\"ì´ ìŠ¤í…: {agent.step_count}\")\n",
    "    \n",
    "    # ì •ë¦¬\n",
    "    env.close()\n",
    "    \n",
    "    return episode_rewards, episode_losses, episode_lengths\n",
    "\n",
    "# í•™ìŠµ ì‹œì‘!\n",
    "print(\"ğŸ í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“‹ {HYPERPARAMETERS['num_episodes']} ì—í”¼ì†Œë“œ í•™ìŠµ ì˜ˆì •\")\n",
    "print(f\"âš¡ ì‚¬ìš© ì¥ì¹˜: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰!\n",
    "episode_rewards, episode_losses, episode_lengths = train_agent()\n",
    "\n",
    "print(\"\\nğŸŠ í•™ìŠµ ì™„ë£Œ! ì•„ë˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---\n",
    "# ## ğŸ“Š í•™ìŠµ ê²°ê³¼ ë¶„ì„\n",
    "# \n",
    "# ì—ì´ì „íŠ¸ì˜ í•™ìŠµ ì„±ê³¼ë¥¼ ì‹œê°í™”í•´ ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¢…í•©ì ì¸ í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„ ìƒì„±\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ğŸï¸ DQN í•™ìŠµ ê²°ê³¼', fontsize=16, fontweight='bold')\n",
    "\n",
    "# ì—í”¼ì†Œë“œ ë³´ìƒ\n",
    "axes[0, 0].plot(episode_rewards, 'b-', alpha=0.6)\n",
    "if len(episode_rewards) >= 10:\n",
    "    # ì´ë™í‰ê·  ì¶”ê°€\n",
    "    window = min(10, len(episode_rewards))\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    axes[0, 0].plot(range(window-1, len(episode_rewards)), moving_avg, 'r-', linewidth=2, label=f'ì´ë™í‰ê· ({window})')\n",
    "    axes[0, 0].legend()\n",
    "axes[0, 0].set_title('ì—í”¼ì†Œë“œ ë³´ìƒ')\n",
    "axes[0, 0].set_xlabel('ì—í”¼ì†Œë“œ')\n",
    "axes[0, 0].set_ylabel('ë³´ìƒ')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ì—í”¼ì†Œë“œ ì†ì‹¤\n",
    "non_zero_losses = [loss for loss in episode_losses if loss > 0]\n",
    "if non_zero_losses:\n",
    "    axes[0, 1].plot(non_zero_losses, 'g-')\n",
    "    axes[0, 1].set_title('í•™ìŠµ ì†ì‹¤')\n",
    "    axes[0, 1].set_xlabel('ì—í”¼ì†Œë“œ (í•™ìŠµ í¬í•¨)')\n",
    "    axes[0, 1].set_ylabel('ì†ì‹¤')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'í•™ìŠµ ë°ì´í„° ì—†ìŒ\\n(ë²„í¼ê°€ ë„ˆë¬´ ì‘ìŒ)', \n",
    "                   ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('í•™ìŠµ ì†ì‹¤')\n",
    "\n",
    "# ì—í”¼ì†Œë“œ ê¸¸ì´\n",
    "axes[1, 0].plot(episode_lengths, 'orange')\n",
    "axes[1, 0].set_title('ì—í”¼ì†Œë“œ ê¸¸ì´')\n",
    "axes[1, 0].set_xlabel('ì—í”¼ì†Œë“œ')\n",
    "axes[1, 0].set_ylabel('ìŠ¤í…')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon ê°ì†Œ\n",
    "epsilons = [HYPERPARAMETERS['epsilon_start'] * (HYPERPARAMETERS['epsilon_decay'] ** i) for i in range(len(episode_rewards))]\n",
    "epsilons = [max(HYPERPARAMETERS['epsilon_end'], eps) for eps in epsilons]\n",
    "axes[1, 1].plot(epsilons, 'purple')\n",
    "axes[1, 1].set_title('Epsilon ê°ì†Œ')\n",
    "axes[1, 1].set_xlabel('ì—í”¼ì†Œë“œ')\n",
    "axes[1, 1].set_ylabel('Epsilon')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# í†µê³„ ì¶œë ¥\n",
    "print(\"ğŸ“ˆ í•™ìŠµ í†µê³„:\")\n",
    "print(f\"   ì—í”¼ì†Œë“œ: {len(episode_rewards)}\")\n",
    "print(f\"   í‰ê·  ë³´ìƒ: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}\")\n",
    "print(f\"   ìµœê³  ë³´ìƒ: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"   ìµœì € ë³´ìƒ: {np.min(episode_rewards):.2f}\")\n",
    "print(f\"   í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´: {np.mean(episode_lengths):.1f} ìŠ¤í…\")\n",
    "print(f\"   ìµœì¢… epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "# ì„±ëŠ¥ ë¶„ì„\n",
    "if len(episode_rewards) >= 20:\n",
    "    early_rewards = np.mean(episode_rewards[:10])\n",
    "    late_rewards = np.mean(episode_rewards[-10:])\n",
    "    improvement = late_rewards - early_rewards\n",
    "    print(f\"\\nğŸ“Š í•™ìŠµ ì§„í–‰ìƒí™©:\")\n",
    "    print(f\"   ì´ˆê¸° ì—í”¼ì†Œë“œ (1-10): {early_rewards:.2f}\")\n",
    "    print(f\"   í›„ê¸° ì—í”¼ì†Œë“œ ({len(episode_rewards)-9}-{len(episode_rewards)}): {late_rewards:.2f}\")\n",
    "    print(f\"   ê°œì„ ë„: {improvement:.2f} ({improvement/abs(early_rewards)*100:.1f}%)\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(\"   ğŸ‰ ì—ì´ì „íŠ¸ê°€ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        print(\"   ğŸ’¡ ë” ë§ì€ ì—í”¼ì†Œë“œë¡œ í•™ìŠµí•˜ê±°ë‚˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ† ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! DQN ì—ì´ì „íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤. ë‹¤ìŒì— í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤ì…ë‹ˆë‹¤:\n",
    "\n",
    "### ğŸ® ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "í•™ìŠµëœ ì—ì´ì „íŠ¸ì˜ ì‹¤ì œ ë™ì‘ì„ í™•ì¸í•˜ë ¤ë©´ ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\n",
    "```bash\n",
    "python ../games/demo_trained_agent.py\n",
    "```\n",
    "\n",
    "### ğŸ”§ ì„±ëŠ¥ ê°œì„ \n",
    "ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ë‹¤ìŒ ê¸°ë²•ë“¤ì„ ì‹œë„í•´ë³´ì„¸ìš”:\n",
    "\n",
    "1. **ë” ì˜¤ë˜ í•™ìŠµ**: `num_episodes`ë¥¼ 500-1000ìœ¼ë¡œ ì¦ê°€\n",
    "2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •**: \n",
    "   - ë‚®ì€ í•™ìŠµë¥  (0.00005)\n",
    "   - ë” í° ë²„í¼ í¬ê¸° (50000)\n",
    "   - ë‹¤ë¥¸ epsilon ê°ì†Œìœ¨ (0.999)\n",
    "3. **ê³ ê¸‰ ê¸°ë²•**:\n",
    "   - Double DQN\n",
    "   - Dueling DQN\n",
    "   - Prioritized Experience Replay\n",
    "\n",
    "### ğŸ“š ë” ì•Œì•„ë³´ê¸°\n",
    "- ì›ë³¸ [DQN ë…¼ë¬¸](https://arxiv.org/abs/1312.5602) ì½ê¸°\n",
    "- Gymnasiumì˜ ë‹¤ë¥¸ í™˜ê²½ë“¤ ì‹œë„\n",
    "- DQN ë³€í˜• êµ¬í˜„\n",
    "\n",
    "### ğŸ’¾ ì‘ì—… ì €ì¥\n",
    "í•™ìŠµëœ ëª¨ë¸ë“¤ì´ `../models/saved_weights/`ì— ì €ì¥ë©ë‹ˆë‹¤:\n",
    "- `dqn_best.pth` - ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
    "- `dqn_final.pth` - í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ëª¨ë¸\n",
    "- `dqn_episode_X.pth` - í•™ìŠµ ì¤‘ ì²´í¬í¬ì¸íŠ¸\n",
    "\n",
    "ì¦ê±°ìš´ ë ˆì´ì‹± ë˜ì„¸ìš”! ğŸï¸ğŸ’¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ìš”ì•½\n",
    "print(\"ğŸ‰ DQN í•™ìŠµ ë…¸íŠ¸ë¶ ì™„ë£Œ!\")\n",
    "print(\"=\"*50)\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì •\")\n",
    "print(\"âœ… DQN ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°\")\n",
    "print(\"âœ… ê²½í—˜ ì¬ìƒ ë²„í¼\")\n",
    "print(\"âœ… ì—ì´ì „íŠ¸ í•™ìŠµ\")\n",
    "print(\"âœ… ê²°ê³¼ ì‹œê°í™”\")\n",
    "print(\"\\nğŸš€ AI ì—ì´ì „íŠ¸ê°€ ë ˆì´ì‹± ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ ë™ì‘ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
