{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# DQN Training for CarRacing Environment ğŸï¸\n\nThis notebook implements a complete DQN training pipeline for the CarRacing-v3 environment. You'll train an AI agent to drive a car using Deep Q-Networks!\n\n## ğŸš€ What You'll Do\n1. **Set up the environment** - CarRacing with preprocessing\n2. **Build the DQN network** - CNN for visual input processing\n3. **Implement training components** - Replay buffer, target network, etc.\n4. **Train the agent** - Watch it learn to drive!\n5. **Analyze results** - Visualize training progress\n\n## âš¡ Quick Start\nMake sure you have:\n- Activated the virtual environment\n- Installed all dependencies\n- Run the tutorial notebook first (recommended)\n\nLet's start training your racing AI! ğŸ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import Tuple, List, Optional, Dict, Any\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸  Using device: {device}\")\n",
    "print(f\"ğŸ Python packages ready!\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "print(f\"ğŸ² Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“‹ Hyperparameters Configuration\n",
    "\n",
    "These hyperparameters control the training process. Feel free to experiment with different values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - Modify these to experiment!\n",
    "HYPERPARAMETERS = {\n",
    "    # Learning parameters\n",
    "    'learning_rate': 0.0001,\n",
    "    'gamma': 0.99,  # Discount factor\n",
    "    \n",
    "    # Exploration parameters\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 32,\n",
    "    'buffer_size': 10000,\n",
    "    'target_update': 1000,  # Update target network every N steps\n",
    "    \n",
    "    # Episode parameters\n",
    "    'num_episodes': 100,  # Start with fewer episodes for notebook\n",
    "    'max_steps_per_episode': 1000,\n",
    "    \n",
    "    # Environment parameters\n",
    "    'frame_stack': 4,\n",
    "    'image_size': (84, 84),\n",
    "    \n",
    "    # Logging\n",
    "    'save_interval': 25,\n",
    "    'log_interval': 5\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Hyperparameters:\")\n",
    "for key, value in HYPERPARAMETERS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nğŸ’¡ Tip: Increase 'num_episodes' to 500+ for better performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸš— Environment Setup\n",
    "\n",
    "Let's create the CarRacing environment with proper preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class CarRacingWrapper:\n    \"\"\"Wrapper for CarRacing environment with preprocessing.\"\"\"\n    \n    def __init__(self, render_mode: Optional[str] = None):\n        \"\"\"\n        Initialize CarRacing environment wrapper.\n        \n        Args:\n            render_mode: Rendering mode ('human', 'rgb_array', or None)\n        \"\"\"\n        self.env = gym.make('CarRacing-v3', render_mode=render_mode)\n        self.frame_stack = HYPERPARAMETERS['frame_stack']\n        self.image_size = HYPERPARAMETERS['image_size']\n        \n        # Frame buffer for stacking\n        self.frames = deque(maxlen=self.frame_stack)\n        \n    def reset(self) -> np.ndarray:\n        \"\"\"Reset environment and return initial stacked frames.\"\"\"\n        obs, info = self.env.reset()\n        \n        # Preprocess initial frame\n        processed_frame = self._preprocess_frame(obs)\n        \n        # Initialize frame stack with repeated first frame\n        for _ in range(self.frame_stack):\n            self.frames.append(processed_frame)\n            \n        return self._get_stacked_frames()\n        \n    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n        \"\"\"\n        Take action and return preprocessed observation.\n        \n        Args:\n            action: Discrete action index\n            \n        Returns:\n            Tuple of (observation, reward, terminated, truncated, info)\n        \"\"\"\n        # Convert discrete action to continuous\n        continuous_action = self._discrete_to_continuous(action)\n        \n        # Take step in environment\n        obs, reward, terminated, truncated, info = self.env.step(continuous_action)\n        \n        # Preprocess and stack frames\n        processed_frame = self._preprocess_frame(obs)\n        self.frames.append(processed_frame)\n        stacked_frames = self._get_stacked_frames()\n        \n        return stacked_frames, reward, terminated, truncated, info\n        \n    def _preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\"Preprocess frame: resize, grayscale, normalize.\"\"\"\n        # Convert to grayscale\n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        \n        # Resize to target size\n        resized_frame = cv2.resize(gray_frame, self.image_size)\n        \n        # Normalize to [0, 1]\n        normalized_frame = resized_frame.astype(np.float32) / 255.0\n        \n        return normalized_frame\n        \n    def _get_stacked_frames(self) -> np.ndarray:\n        \"\"\"Get stacked frames as numpy array.\"\"\"\n        return np.array(list(self.frames))\n        \n    def _discrete_to_continuous(self, action: int) -> np.ndarray:\n        \"\"\"Convert discrete action to continuous action space.\"\"\"\n        if action == 0:     # Turn left\n            return np.array([-0.5, 0.3, 0.0])\n        elif action == 1:   # Go straight\n            return np.array([0.0, 0.5, 0.0])\n        elif action == 2:   # Turn right\n            return np.array([0.5, 0.3, 0.0])\n        elif action == 3:   # Brake\n            return np.array([0.0, 0.0, 0.8])\n        else:\n            return np.array([0.0, 0.0, 0.0])\n            \n    def close(self):\n        \"\"\"Close the environment.\"\"\"\n        self.env.close()\n\n# Test the environment\nprint(\"ğŸš— Testing CarRacing Environment...\")\ntest_env = CarRacingWrapper()\ntest_obs = test_env.reset()\nprint(f\"âœ… Environment initialized successfully!\")\nprint(f\"   Observation shape: {test_obs.shape}\")\nprint(f\"   Action space: 4 discrete actions (left, straight, right, brake)\")\ntest_env.close()\ndel test_env"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§  DQN Network Architecture\n",
    "\n",
    "Let's build our CNN-based Deep Q-Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"CNN-based Deep Q-Network for CarRacing.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_dim: int = 4, input_channels: int = 4):\n",
    "        \"\"\"\n",
    "        Initialize DQN network.\n",
    "        \n",
    "        Args:\n",
    "            action_dim: Number of discrete actions\n",
    "            input_channels: Number of input channels (frame stack)\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        # Calculate conv output size\n",
    "        self._conv_output_size = self._get_conv_output_size((input_channels, 84, 84))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._conv_output_size, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _get_conv_output_size(self, input_shape: Tuple[int, int, int]) -> int:\n",
    "        \"\"\"Calculate output size after conv layers.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            dummy_output = self._forward_conv(dummy_input)\n",
    "            return dummy_output.numel()\n",
    "            \n",
    "    def _forward_conv(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through conv layers only.\"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return x.view(x.size(0), -1)\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "                    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through network.\"\"\"\n",
    "        x = self._forward_conv(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create and analyze the network\n",
    "dqn = DQN(action_dim=4, input_channels=4).to(device)\n",
    "total_params = sum(p.numel() for p in dqn.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"ğŸ§  DQN Network Created!\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Network size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 4, 84, 84).to(device)\n",
    "with torch.no_grad():\n",
    "    output = dqn(dummy_input)\n",
    "print(f\"   Input shape: {dummy_input.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"âœ… Network test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ—ƒï¸ Experience Replay Buffer\n",
    "\n",
    "The replay buffer stores and samples experiences for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for storing transitions.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add transition to buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        \"\"\"Sample batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.BoolTensor(dones)\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Create replay buffer\n",
    "replay_buffer = ReplayBuffer(HYPERPARAMETERS['buffer_size'])\n",
    "print(f\"ğŸ—ƒï¸  Replay buffer created with capacity: {HYPERPARAMETERS['buffer_size']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¤– DQN Agent\n",
    "\n",
    "Let's create our DQN agent that combines all the components!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with all training components.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device = device\n",
    "        self.action_dim = 4  # left, straight, right, brake\n",
    "        \n",
    "        # Networks\n",
    "        self.main_network = DQN(self.action_dim).to(device)\n",
    "        self.target_network = DQN(self.action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.main_network.parameters(), \n",
    "            lr=HYPERPARAMETERS['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(HYPERPARAMETERS['buffer_size'])\n",
    "        \n",
    "        # Exploration strategy\n",
    "        self.epsilon = HYPERPARAMETERS['epsilon_start']\n",
    "        self.epsilon_decay = HYPERPARAMETERS['epsilon_decay']\n",
    "        self.epsilon_min = HYPERPARAMETERS['epsilon_end']\n",
    "        \n",
    "        # Training counters\n",
    "        self.step_count = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "    def select_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.main_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "            \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def update(self) -> Optional[float]:\n",
    "        \"\"\"Update network using batch from replay buffer.\"\"\"\n",
    "        if len(self.replay_buffer) < HYPERPARAMETERS['batch_size']:\n",
    "            return None\n",
    "            \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = \\\n",
    "            self.replay_buffer.sample(HYPERPARAMETERS['batch_size'])\n",
    "            \n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_values = self.main_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Next Q-values from target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            targets = rewards + (HYPERPARAMETERS['gamma'] * next_q_values * (~dones))\n",
    "            \n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(current_q_values.squeeze(), targets)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.main_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update step counter\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Update target network\n",
    "        if self.step_count % HYPERPARAMETERS['target_update'] == 0:\n",
    "            self.update_target_network()\n",
    "            \n",
    "        return loss.item()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network with main network weights.\"\"\"\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Update epsilon for next episode.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.episode_count += 1\n",
    "\n",
    "# Create the agent\n",
    "agent = DQNAgent(device)\n",
    "print(f\"ğŸ¤– DQN Agent created!\")\n",
    "print(f\"   Main network parameters: {sum(p.numel() for p in agent.main_network.parameters()):,}\")\n",
    "print(f\"   Target network parameters: {sum(p.numel() for p in agent.target_network.parameters()):,}\")\n",
    "print(f\"   Initial epsilon: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ‹ï¸ Training Loop\n",
    "\n",
    "Now let's train our agent! This is where the magic happens. ğŸª„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(num_episodes: int = HYPERPARAMETERS['num_episodes']):\n",
    "    \"\"\"Train the DQN agent.\"\"\"\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = CarRacingWrapper()\n",
    "    \n",
    "    # Training statistics\n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    # Create directories for saving\n",
    "    models_dir = Path(\"../models/saved_weights\")\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"ğŸš€ Starting training for {num_episodes} episodes...\")\n",
    "    print(f\"ğŸ“Š Progress will be displayed every {HYPERPARAMETERS['log_interval']} episodes\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    # Training loop with progress bar\n",
    "    progress_bar = tqdm(range(num_episodes), desc=\"Training\")\n",
    "    \n",
    "    for episode in progress_bar:\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        episode_loss_list = []\n",
    "        step = 0\n",
    "        \n",
    "        # Episode loop\n",
    "        for step in range(HYPERPARAMETERS['max_steps_per_episode']):\n",
    "            # Select and take action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            done = terminated or truncated\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update agent\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_loss_list.append(loss)\n",
    "                \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Update statistics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        avg_loss = np.mean(episode_loss_list) if episode_loss_list else 0.0\n",
    "        episode_losses.append(avg_loss)\n",
    "        \n",
    "        # Update exploration\n",
    "        agent.update_epsilon()\n",
    "        \n",
    "        # Update progress bar\n",
    "        recent_rewards = episode_rewards[-10:] if len(episode_rewards) >= 10 else episode_rewards\n",
    "        avg_reward = np.mean(recent_rewards)\n",
    "        progress_bar.set_postfix({\n",
    "            'Reward': f'{episode_reward:.1f}',\n",
    "            'Avg': f'{avg_reward:.1f}',\n",
    "            'Îµ': f'{agent.epsilon:.3f}'\n",
    "        })\n",
    "        \n",
    "        # Logging\n",
    "        if episode % HYPERPARAMETERS['log_interval'] == 0 and episode > 0:\n",
    "            print(f\"\\nEpisode {episode:4d} | \"\n",
    "                  f\"Reward: {episode_reward:8.2f} | \"\n",
    "                  f\"Avg Reward: {avg_reward:8.2f} | \"\n",
    "                  f\"Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.4f} | \"\n",
    "                  f\"Buffer: {len(agent.replay_buffer)}\")\n",
    "                  \n",
    "        # Save model\n",
    "        if episode % HYPERPARAMETERS['save_interval'] == 0 and episode > 0:\n",
    "            model_path = models_dir / f\"dqn_episode_{episode}.pth\"\n",
    "            torch.save({\n",
    "                'main_network': agent.main_network.state_dict(),\n",
    "                'target_network': agent.target_network.state_dict(),\n",
    "                'optimizer': agent.optimizer.state_dict(),\n",
    "                'epsilon': agent.epsilon,\n",
    "                'step_count': agent.step_count,\n",
    "                'episode_count': agent.episode_count\n",
    "            }, model_path)\n",
    "            \n",
    "            # Save best model\n",
    "            if episode_reward > best_reward:\n",
    "                best_reward = episode_reward\n",
    "                best_model_path = models_dir / \"dqn_best.pth\"\n",
    "                torch.save({\n",
    "                    'main_network': agent.main_network.state_dict(),\n",
    "                    'target_network': agent.target_network.state_dict(),\n",
    "                    'optimizer': agent.optimizer.state_dict(),\n",
    "                    'epsilon': agent.epsilon,\n",
    "                    'step_count': agent.step_count,\n",
    "                    'episode_count': agent.episode_count\n",
    "                }, best_model_path)\n",
    "                print(f\"ğŸ’¾ New best model saved! Reward: {best_reward:.2f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = models_dir / \"dqn_final.pth\"\n",
    "    torch.save({\n",
    "        'main_network': agent.main_network.state_dict(),\n",
    "        'target_network': agent.target_network.state_dict(),\n",
    "        'optimizer': agent.optimizer.state_dict(),\n",
    "        'epsilon': agent.epsilon,\n",
    "        'step_count': agent.step_count,\n",
    "        'episode_count': agent.episode_count\n",
    "    }, final_model_path)\n",
    "    \n",
    "    # Training summary\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total episodes: {len(episode_rewards)}\")\n",
    "    print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"Average reward: {np.mean(episode_rewards):.2f}\")\n",
    "    print(f\"Best reward: {np.max(episode_rewards):.2f}\")\n",
    "    print(f\"Final epsilon: {agent.epsilon:.4f}\")\n",
    "    print(f\"Total steps: {agent.step_count}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    env.close()\n",
    "    \n",
    "    return episode_rewards, episode_losses, episode_lengths\n",
    "\n",
    "# Start training!\n",
    "print(\"ğŸ Ready to start training!\")\n",
    "print(f\"ğŸ“‹ Training for {HYPERPARAMETERS['num_episodes']} episodes\")\n",
    "print(f\"âš¡ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training!\n",
    "episode_rewards, episode_losses, episode_lengths = train_agent()\n",
    "\n",
    "print(\"\\nğŸŠ Training finished! Check the results below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Training Results Analysis\n",
    "\n",
    "Let's visualize how our agent performed during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ğŸï¸ DQN Training Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(episode_rewards, 'b-', alpha=0.6)\n",
    "if len(episode_rewards) >= 10:\n",
    "    # Add moving average\n",
    "    window = min(10, len(episode_rewards))\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    axes[0, 0].plot(range(window-1, len(episode_rewards)), moving_avg, 'r-', linewidth=2, label=f'MA({window})')\n",
    "    axes[0, 0].legend()\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Episode losses\n",
    "non_zero_losses = [loss for loss in episode_losses if loss > 0]\n",
    "if non_zero_losses:\n",
    "    axes[0, 1].plot(non_zero_losses, 'g-')\n",
    "    axes[0, 1].set_title('Training Loss')\n",
    "    axes[0, 1].set_xlabel('Episode (with training)')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No training data\\n(buffer too small)', \n",
    "                   ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('Training Loss')\n",
    "\n",
    "# Episode lengths\n",
    "axes[1, 0].plot(episode_lengths, 'orange')\n",
    "axes[1, 0].set_title('Episode Lengths')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Steps')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon decay\n",
    "epsilons = [HYPERPARAMETERS['epsilon_start'] * (HYPERPARAMETERS['epsilon_decay'] ** i) for i in range(len(episode_rewards))]\n",
    "epsilons = [max(HYPERPARAMETERS['epsilon_end'], eps) for eps in epsilons]\n",
    "axes[1, 1].plot(epsilons, 'purple')\n",
    "axes[1, 1].set_title('Epsilon Decay')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Epsilon')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"ğŸ“ˆ Training Statistics:\")\n",
    "print(f\"   Episodes: {len(episode_rewards)}\")\n",
    "print(f\"   Average reward: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}\")\n",
    "print(f\"   Best reward: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"   Worst reward: {np.min(episode_rewards):.2f}\")\n",
    "print(f\"   Average episode length: {np.mean(episode_lengths):.1f} steps\")\n",
    "print(f\"   Final epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "# Performance analysis\n",
    "if len(episode_rewards) >= 20:\n",
    "    early_rewards = np.mean(episode_rewards[:10])\n",
    "    late_rewards = np.mean(episode_rewards[-10:])\n",
    "    improvement = late_rewards - early_rewards\n",
    "    print(f\"\\nğŸ“Š Learning Progress:\")\n",
    "    print(f\"   Early episodes (1-10): {early_rewards:.2f}\")\n",
    "    print(f\"   Late episodes ({len(episode_rewards)-9}-{len(episode_rewards)}): {late_rewards:.2f}\")\n",
    "    print(f\"   Improvement: {improvement:.2f} ({improvement/abs(early_rewards)*100:.1f}%)\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(\"   ğŸ‰ Your agent is learning!\")\n",
    "    else:\n",
    "        print(\"   ğŸ’¡ Try training for more episodes or tuning hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ† Next Steps\n",
    "\n",
    "Congratulations! You've successfully trained a DQN agent. Here's what you can do next:\n",
    "\n",
    "### ğŸ® Test Your Agent\n",
    "Run the demo script to see your trained agent in action:\n",
    "```bash\n",
    "python ../games/demo_trained_agent.py\n",
    "```\n",
    "\n",
    "### ğŸ”§ Improve Performance\n",
    "Try these techniques to get better results:\n",
    "\n",
    "1. **Train Longer**: Increase `num_episodes` to 500-1000\n",
    "2. **Tune Hyperparameters**: \n",
    "   - Lower learning rate (0.00005)\n",
    "   - Larger buffer size (50000)\n",
    "   - Different epsilon decay (0.999)\n",
    "3. **Advanced Techniques**:\n",
    "   - Double DQN\n",
    "   - Dueling DQN\n",
    "   - Prioritized Experience Replay\n",
    "\n",
    "### ğŸ“š Learn More\n",
    "- Read the original [DQN paper](https://arxiv.org/abs/1312.5602)\n",
    "- Try other environments from Gymnasium\n",
    "- Implement DQN variants\n",
    "\n",
    "### ğŸ’¾ Save Your Work\n",
    "Your trained models are saved in `../models/saved_weights/`:\n",
    "- `dqn_best.pth` - Best performing model\n",
    "- `dqn_final.pth` - Final model after training\n",
    "- `dqn_episode_X.pth` - Checkpoints during training\n",
    "\n",
    "Happy racing! ğŸï¸ğŸ’¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"ğŸ‰ DQN Training Notebook Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(\"âœ… Environment setup\")\n",
    "print(\"âœ… DQN network architecture\")\n",
    "print(\"âœ… Experience replay buffer\")\n",
    "print(\"âœ… Agent training\")\n",
    "print(\"âœ… Results visualization\")\n",
    "print(\"\\nğŸš€ Your AI agent is ready to race!\")\n",
    "print(\"Run the demo script to see it in action.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}