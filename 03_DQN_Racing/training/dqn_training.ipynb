{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarRacing 환경을 위한 DQN 학습\n",
    "\n",
    "이 노트북은 CarRacing-v3 환경을 위한 완전한 DQN 학습 파이프라인을 구현합니다. Deep Q-Networks를 사용하여 자동차를 운전하는 AI 에이전트를 학습시킵니다!\n",
    "\n",
    "## 수행할 작업\n",
    "1. **환경 설정** - 전처리가 포함된 CarRacing\n",
    "2. **DQN 네트워크 구축** - 시각적 입력 처리를 위한 CNN\n",
    "3. **학습 구성요소 구현** - Replay buffer, target network 등\n",
    "4. **에이전트 학습** - 운전을 학습하는 과정 관찰!\n",
    "5. **결과 분석** - 학습 진행 상황 시각화\n",
    "\n",
    "## 빠른 시작\n",
    "다음을 확인하세요:\n",
    "- 가상 환경 활성화\n",
    "- 모든 종속성 설치\n",
    "- 튜토리얼 노트북을 먼저 실행 (권장)\n",
    "\n",
    "레이싱 AI 학습을 시작합시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모든 라이브러리 import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import Tuple, List, Optional, Dict, Any\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 그래프 스타일 설정\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# 장치 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🖥️  사용 장치: {device}\")\n",
    "print(f\"🐍 Python 패키지 준비 완료!\")\n",
    "\n",
    "# 재현 가능성을 위한 랜덤 시드 설정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "print(f\"🎲 재현 가능성을 위한 랜덤 시드 설정 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 하이퍼파라미터 설정\n",
    "\n",
    "이 하이퍼파라미터들은 학습 프로세스를 제어합니다. 다양한 값으로 실험해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 - 실험을 위해 이 값들을 수정해보세요!\n",
    "HYPERPARAMETERS = {\n",
    "    # 학습 파라미터\n",
    "    'learning_rate': 0.0001,\n",
    "    'gamma': 0.99,  # 할인 인자\n",
    "    \n",
    "    # 탐험 파라미터\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    \n",
    "    # 학습 파라미터\n",
    "    'batch_size': 32,\n",
    "    'buffer_size': 10000,\n",
    "    'target_update': 1000,  # N 스텝마다 target network 업데이트\n",
    "    \n",
    "    # 에피소드 파라미터\n",
    "    'num_episodes': 100,  # 노트북용으로 적은 에피소드로 시작\n",
    "    'max_steps_per_episode': 1000,\n",
    "    \n",
    "    # 환경 파라미터\n",
    "    'frame_stack': 4,\n",
    "    'image_size': (84, 84),\n",
    "    \n",
    "    # 로깅\n",
    "    'save_interval': 25,\n",
    "    'log_interval': 5\n",
    "}\n",
    "\n",
    "print(\"📊 하이퍼파라미터:\")\n",
    "for key, value in HYPERPARAMETERS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\n💡 팁: 더 나은 성능을 위해 'num_episodes'를 500+로 증가시키세요!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 환경 설정\n",
    "\n",
    "적절한 전처리와 함께 CarRacing 환경을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRacingWrapper:\n",
    "    \"\"\"전처리가 포함된 CarRacing 환경 래퍼\"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        CarRacing 환경 래퍼 초기화\n",
    "        \n",
    "        Args:\n",
    "            render_mode: 렌더링 모드 ('human', 'rgb_array', 또는 None)\n",
    "        \"\"\"\n",
    "        self.env = gym.make('CarRacing-v3', render_mode=render_mode)\n",
    "        self.frame_stack = HYPERPARAMETERS['frame_stack']\n",
    "        self.image_size = HYPERPARAMETERS['image_size']\n",
    "        \n",
    "        # 프레임 스태킹을 위한 버퍼\n",
    "        self.frames = deque(maxlen=self.frame_stack)\n",
    "        \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"환경 리셋 및 초기 스택 프레임 반환\"\"\"\n",
    "        obs, info = self.env.reset()\n",
    "        \n",
    "        # 초기 프레임 전처리\n",
    "        processed_frame = self._preprocess_frame(obs)\n",
    "        \n",
    "        # 첫 프레임을 반복하여 프레임 스택 초기화\n",
    "        for _ in range(self.frame_stack):\n",
    "            self.frames.append(processed_frame)\n",
    "            \n",
    "        return self._get_stacked_frames()\n",
    "        \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"\n",
    "        행동을 수행하고 전처리된 관측값 반환\n",
    "        \n",
    "        Args:\n",
    "            action: 이산 행동 인덱스\n",
    "            \n",
    "        Returns:\n",
    "            (관측값, 보상, 종료여부, 잘림여부, 정보) 튜플\n",
    "        \"\"\"\n",
    "        # 이산 행동을 연속 행동으로 변환\n",
    "        continuous_action = self._discrete_to_continuous(action)\n",
    "        \n",
    "        # 환경에서 스텝 수행\n",
    "        obs, reward, terminated, truncated, info = self.env.step(continuous_action)\n",
    "        \n",
    "        # 프레임 전처리 및 스택\n",
    "        processed_frame = self._preprocess_frame(obs)\n",
    "        self.frames.append(processed_frame)\n",
    "        stacked_frames = self._get_stacked_frames()\n",
    "        \n",
    "        return stacked_frames, reward, terminated, truncated, info\n",
    "        \n",
    "    def _preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"프레임 전처리: 크기 조정, 그레이스케일, 정규화\"\"\"\n",
    "        # 그레이스케일로 변환\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # 타겟 크기로 리사이즈\n",
    "        resized_frame = cv2.resize(gray_frame, self.image_size)\n",
    "        \n",
    "        # [0, 1] 범위로 정규화\n",
    "        normalized_frame = resized_frame.astype(np.float32) / 255.0\n",
    "        \n",
    "        return normalized_frame\n",
    "        \n",
    "    def _get_stacked_frames(self) -> np.ndarray:\n",
    "        \"\"\"스택된 프레임을 numpy 배열로 반환\"\"\"\n",
    "        return np.array(list(self.frames))\n",
    "        \n",
    "    def _discrete_to_continuous(self, action: int) -> np.ndarray:\n",
    "        \"\"\"이산 행동을 연속 행동 공간으로 변환\"\"\"\n",
    "        if action == 0:     # 왼쪽 회전\n",
    "            return np.array([-0.5, 0.3, 0.0])\n",
    "        elif action == 1:   # 직진\n",
    "            return np.array([0.0, 0.5, 0.0])\n",
    "        elif action == 2:   # 오른쪽 회전\n",
    "            return np.array([0.5, 0.3, 0.0])\n",
    "        elif action == 3:   # 브레이크\n",
    "            return np.array([0.0, 0.0, 0.8])\n",
    "        else:\n",
    "            return np.array([0.0, 0.0, 0.0])\n",
    "            \n",
    "    def close(self):\n",
    "        \"\"\"환경 종료\"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "# 환경 테스트\n",
    "print(\"🚗 CarRacing 환경 테스트 중...\")\n",
    "test_env = CarRacingWrapper()\n",
    "test_obs = test_env.reset()\n",
    "print(f\"✅ 환경 초기화 성공!\")\n",
    "print(f\"   관측값 형태: {test_obs.shape}\")\n",
    "print(f\"   행동 공간: 4개 이산 행동 (왼쪽, 직진, 오른쪽, 브레이크)\")\n",
    "test_env.close()\n",
    "del test_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DQN 네트워크 구조\n",
    "\n",
    "CNN 기반 Deep Q-Network를 구축합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"CarRacing을 위한 CNN 기반 Deep Q-Network\"\"\"\n",
    "    \n",
    "    def __init__(self, action_dim: int = 4, input_channels: int = 4):\n",
    "        \"\"\"\n",
    "        DQN 네트워크 초기화\n",
    "        \n",
    "        Args:\n",
    "            action_dim: 이산 행동의 개수\n",
    "            input_channels: 입력 채널 수 (프레임 스택)\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # 합성곱 레이어\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        # Conv 출력 크기 계산\n",
    "        self._conv_output_size = self._get_conv_output_size((input_channels, 84, 84))\n",
    "        \n",
    "        # 완전 연결 레이어\n",
    "        self.fc1 = nn.Linear(self._conv_output_size, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _get_conv_output_size(self, input_shape: Tuple[int, int, int]) -> int:\n",
    "        \"\"\"Conv 레이어 통과 후 출력 크기 계산\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            dummy_output = self._forward_conv(dummy_input)\n",
    "            return dummy_output.numel()\n",
    "            \n",
    "    def _forward_conv(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Conv 레이어만 통과하는 forward pass\"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return x.view(x.size(0), -1)\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"네트워크 가중치 초기화\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "                    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"네트워크 forward pass\"\"\"\n",
    "        x = self._forward_conv(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 네트워크 생성 및 분석\n",
    "dqn = DQN(action_dim=4, input_channels=4).to(device)\n",
    "total_params = sum(p.numel() for p in dqn.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"🧠 DQN 네트워크 생성 완료!\")\n",
    "print(f\"   총 파라미터 수: {total_params:,}\")\n",
    "print(f\"   네트워크 크기: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Forward pass 테스트\n",
    "dummy_input = torch.randn(1, 4, 84, 84).to(device)\n",
    "with torch.no_grad():\n",
    "    output = dqn(dummy_input)\n",
    "print(f\"   입력 형태: {dummy_input.shape}\")\n",
    "print(f\"   출력 형태: {output.shape}\")\n",
    "print(f\"✅ 네트워크 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experience Replay Buffer\n",
    "\n",
    "Replay buffer는 학습을 위한 경험을 저장하고 샘플링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"전환을 저장하기 위한 경험 재생 버퍼\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"버퍼에 전환 추가\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        \"\"\"전환 배치 샘플링\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.BoolTensor(dones)\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Replay buffer 생성\n",
    "replay_buffer = ReplayBuffer(HYPERPARAMETERS['buffer_size'])\n",
    "print(f\"🗃️  Replay buffer 생성 완료 (용량: {HYPERPARAMETERS['buffer_size']:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DQN 에이전트\n",
    "\n",
    "모든 구성요소를 결합한 DQN 에이전트를 생성합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"모든 학습 구성요소를 포함한 DQN 에이전트\"\"\"\n",
    "    \n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device = device\n",
    "        self.action_dim = 4  # 왼쪽, 직진, 오른쪽, 브레이크\n",
    "        \n",
    "        # 네트워크\n",
    "        self.main_network = DQN(self.action_dim).to(device)\n",
    "        self.target_network = DQN(self.action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        \n",
    "        # 옵티마이저\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.main_network.parameters(), \n",
    "            lr=HYPERPARAMETERS['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(HYPERPARAMETERS['buffer_size'])\n",
    "        \n",
    "        # 탐험 전략\n",
    "        self.epsilon = HYPERPARAMETERS['epsilon_start']\n",
    "        self.epsilon_decay = HYPERPARAMETERS['epsilon_decay']\n",
    "        self.epsilon_min = HYPERPARAMETERS['epsilon_end']\n",
    "        \n",
    "        # 학습 카운터\n",
    "        self.step_count = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "    def select_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"Epsilon-greedy 정책으로 행동 선택\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.main_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "            \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Replay buffer에 전환 저장\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def update(self) -> Optional[float]:\n",
    "        \"\"\"Replay buffer의 배치를 사용하여 네트워크 업데이트\"\"\"\n",
    "        if len(self.replay_buffer) < HYPERPARAMETERS['batch_size']:\n",
    "            return None\n",
    "            \n",
    "        # 배치 샘플링\n",
    "        states, actions, rewards, next_states, dones = \\\n",
    "            self.replay_buffer.sample(HYPERPARAMETERS['batch_size'])\n",
    "            \n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # 현재 Q-값\n",
    "        current_q_values = self.main_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target network의 다음 Q-값\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            targets = rewards + (HYPERPARAMETERS['gamma'] * next_q_values * (~dones))\n",
    "            \n",
    "        # 손실 계산\n",
    "        loss = F.smooth_l1_loss(current_q_values.squeeze(), targets)\n",
    "        \n",
    "        # 최적화\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.main_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 스텝 카운터 업데이트\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Target network 업데이트\n",
    "        if self.step_count % HYPERPARAMETERS['target_update'] == 0:\n",
    "            self.update_target_network()\n",
    "            \n",
    "        return loss.item()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Main network 가중치로 target network 업데이트\"\"\"\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"다음 에피소드를 위한 epsilon 업데이트\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.episode_count += 1\n",
    "\n",
    "# 에이전트 생성\n",
    "agent = DQNAgent(device)\n",
    "print(f\"🤖 DQN 에이전트 생성 완료!\")\n",
    "print(f\"   Main network 파라미터: {sum(p.numel() for p in agent.main_network.parameters()):,}\")\n",
    "print(f\"   Target network 파라미터: {sum(p.numel() for p in agent.target_network.parameters()):,}\")\n",
    "print(f\"   초기 epsilon: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 학습 루프\n",
    "\n",
    "이제 에이전트를 학습시킵니다! 여기서 마법이 일어납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(num_episodes: int = HYPERPARAMETERS['num_episodes']):\n",
    "    \"\"\"DQN 에이전트 학습\"\"\"\n",
    "    \n",
    "    # 환경 초기화\n",
    "    env = CarRacingWrapper()\n",
    "    \n",
    "    # 학습 통계\n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    # 저장용 디렉토리 생성\n",
    "    models_dir = Path(\"../models/saved_weights\")\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"🚀 {num_episodes} 에피소드 학습 시작...\")\n",
    "    print(f\"📊 {HYPERPARAMETERS['log_interval']} 에피소드마다 진행상황 표시\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    # 진행바와 함께 학습 루프\n",
    "    progress_bar = tqdm(range(num_episodes), desc=\"학습 중\")\n",
    "    \n",
    "    for episode in progress_bar:\n",
    "        # 환경 리셋\n",
    "        state = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        episode_loss_list = []\n",
    "        step = 0\n",
    "        \n",
    "        # 에피소드 루프\n",
    "        for step in range(HYPERPARAMETERS['max_steps_per_episode']):\n",
    "            # 행동 선택 및 수행\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            # 전환 저장\n",
    "            done = terminated or truncated\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # 에이전트 업데이트\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_loss_list.append(loss)\n",
    "                \n",
    "            # 상태 및 보상 업데이트\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # 통계 업데이트\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        avg_loss = np.mean(episode_loss_list) if episode_loss_list else 0.0\n",
    "        episode_losses.append(avg_loss)\n",
    "        \n",
    "        # 탐험 업데이트\n",
    "        agent.update_epsilon()\n",
    "        \n",
    "        # 진행바 업데이트\n",
    "        recent_rewards = episode_rewards[-10:] if len(episode_rewards) >= 10 else episode_rewards\n",
    "        avg_reward = np.mean(recent_rewards)\n",
    "        progress_bar.set_postfix({\n",
    "            '보상': f'{episode_reward:.1f}',\n",
    "            '평균': f'{avg_reward:.1f}',\n",
    "            'ε': f'{agent.epsilon:.3f}'\n",
    "        })\n",
    "        \n",
    "        # 로깅\n",
    "        if episode % HYPERPARAMETERS['log_interval'] == 0 and episode > 0:\n",
    "            print(f\"\\n에피소드 {episode:4d} | \"\n",
    "                  f\"보상: {episode_reward:8.2f} | \"\n",
    "                  f\"평균 보상: {avg_reward:8.2f} | \"\n",
    "                  f\"손실: {avg_loss:.4f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.4f} | \"\n",
    "                  f\"버퍼: {len(agent.replay_buffer)}\")\n",
    "                  \n",
    "        # 모델 저장\n",
    "        if episode % HYPERPARAMETERS['save_interval'] == 0 and episode > 0:\n",
    "            model_path = models_dir / f\"dqn_episode_{episode}.pth\"\n",
    "            torch.save({\n",
    "                'main_network': agent.main_network.state_dict(),\n",
    "                'target_network': agent.target_network.state_dict(),\n",
    "                'optimizer': agent.optimizer.state_dict(),\n",
    "                'epsilon': agent.epsilon,\n",
    "                'step_count': agent.step_count,\n",
    "                'episode_count': agent.episode_count\n",
    "            }, model_path)\n",
    "            \n",
    "            # 최고 모델 저장\n",
    "            if episode_reward > best_reward:\n",
    "                best_reward = episode_reward\n",
    "                best_model_path = models_dir / \"dqn_best.pth\"\n",
    "                torch.save({\n",
    "                    'main_network': agent.main_network.state_dict(),\n",
    "                    'target_network': agent.target_network.state_dict(),\n",
    "                    'optimizer': agent.optimizer.state_dict(),\n",
    "                    'epsilon': agent.epsilon,\n",
    "                    'step_count': agent.step_count,\n",
    "                    'episode_count': agent.episode_count\n",
    "                }, best_model_path)\n",
    "                print(f\"💾 새로운 최고 모델 저장! 보상: {best_reward:.2f}\")\n",
    "    \n",
    "    # 최종 모델 저장\n",
    "    final_model_path = models_dir / \"dqn_final.pth\"\n",
    "    torch.save({\n",
    "        'main_network': agent.main_network.state_dict(),\n",
    "        'target_network': agent.target_network.state_dict(),\n",
    "        'optimizer': agent.optimizer.state_dict(),\n",
    "        'epsilon': agent.epsilon,\n",
    "        'step_count': agent.step_count,\n",
    "        'episode_count': agent.episode_count\n",
    "    }, final_model_path)\n",
    "    \n",
    "    # 학습 요약\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎉 학습 완료!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"총 에피소드: {len(episode_rewards)}\")\n",
    "    print(f\"총 시간: {total_time/60:.1f} 분\")\n",
    "    print(f\"평균 보상: {np.mean(episode_rewards):.2f}\")\n",
    "    print(f\"최고 보상: {np.max(episode_rewards):.2f}\")\n",
    "    print(f\"최종 epsilon: {agent.epsilon:.4f}\")\n",
    "    print(f\"총 스텝: {agent.step_count}\")\n",
    "    \n",
    "    # 정리\n",
    "    env.close()\n",
    "    \n",
    "    return episode_rewards, episode_losses, episode_lengths\n",
    "\n",
    "# 학습 시작!\n",
    "print(\"🏁 학습 준비 완료!\")\n",
    "print(f\"📋 {HYPERPARAMETERS['num_episodes']} 에피소드 학습 예정\")\n",
    "print(f\"⚡ 사용 장치: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실행!\n",
    "episode_rewards, episode_losses, episode_lengths = train_agent()\n",
    "\n",
    "print(\"\\n🎊 학습 완료! 아래 결과를 확인하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---\n",
    "# ## 📊 학습 결과 분석\n",
    "# \n",
    "# 에이전트의 학습 성과를 시각화해 봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합적인 학습 결과 그래프 생성\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('🏎️ DQN 학습 결과', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 에피소드 보상\n",
    "axes[0, 0].plot(episode_rewards, 'b-', alpha=0.6)\n",
    "if len(episode_rewards) >= 10:\n",
    "    # 이동평균 추가\n",
    "    window = min(10, len(episode_rewards))\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    axes[0, 0].plot(range(window-1, len(episode_rewards)), moving_avg, 'r-', linewidth=2, label=f'이동평균({window})')\n",
    "    axes[0, 0].legend()\n",
    "axes[0, 0].set_title('에피소드 보상')\n",
    "axes[0, 0].set_xlabel('에피소드')\n",
    "axes[0, 0].set_ylabel('보상')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 에피소드 손실\n",
    "non_zero_losses = [loss for loss in episode_losses if loss > 0]\n",
    "if non_zero_losses:\n",
    "    axes[0, 1].plot(non_zero_losses, 'g-')\n",
    "    axes[0, 1].set_title('학습 손실')\n",
    "    axes[0, 1].set_xlabel('에피소드 (학습 포함)')\n",
    "    axes[0, 1].set_ylabel('손실')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, '학습 데이터 없음\\n(버퍼가 너무 작음)', \n",
    "                   ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('학습 손실')\n",
    "\n",
    "# 에피소드 길이\n",
    "axes[1, 0].plot(episode_lengths, 'orange')\n",
    "axes[1, 0].set_title('에피소드 길이')\n",
    "axes[1, 0].set_xlabel('에피소드')\n",
    "axes[1, 0].set_ylabel('스텝')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon 감소\n",
    "epsilons = [HYPERPARAMETERS['epsilon_start'] * (HYPERPARAMETERS['epsilon_decay'] ** i) for i in range(len(episode_rewards))]\n",
    "epsilons = [max(HYPERPARAMETERS['epsilon_end'], eps) for eps in epsilons]\n",
    "axes[1, 1].plot(epsilons, 'purple')\n",
    "axes[1, 1].set_title('Epsilon 감소')\n",
    "axes[1, 1].set_xlabel('에피소드')\n",
    "axes[1, 1].set_ylabel('Epsilon')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 통계 출력\n",
    "print(\"📈 학습 통계:\")\n",
    "print(f\"   에피소드: {len(episode_rewards)}\")\n",
    "print(f\"   평균 보상: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "print(f\"   최고 보상: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"   최저 보상: {np.min(episode_rewards):.2f}\")\n",
    "print(f\"   평균 에피소드 길이: {np.mean(episode_lengths):.1f} 스텝\")\n",
    "print(f\"   최종 epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "# 성능 분석\n",
    "if len(episode_rewards) >= 20:\n",
    "    early_rewards = np.mean(episode_rewards[:10])\n",
    "    late_rewards = np.mean(episode_rewards[-10:])\n",
    "    improvement = late_rewards - early_rewards\n",
    "    print(f\"\\n📊 학습 진행상황:\")\n",
    "    print(f\"   초기 에피소드 (1-10): {early_rewards:.2f}\")\n",
    "    print(f\"   후기 에피소드 ({len(episode_rewards)-9}-{len(episode_rewards)}): {late_rewards:.2f}\")\n",
    "    print(f\"   개선도: {improvement:.2f} ({improvement/abs(early_rewards)*100:.1f}%)\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(\"   🎉 에이전트가 학습하고 있습니다!\")\n",
    "    else:\n",
    "        print(\"   💡 더 많은 에피소드로 학습하거나 하이퍼파라미터를 조정해보세요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🏆 다음 단계\n",
    "\n",
    "축하합니다! DQN 에이전트를 성공적으로 학습시켰습니다. 다음에 할 수 있는 것들입니다:\n",
    "\n",
    "### 🎮 에이전트 테스트\n",
    "학습된 에이전트의 실제 동작을 확인하려면 데모 스크립트를 실행하세요:\n",
    "```bash\n",
    "python ../games/demo_trained_agent.py\n",
    "```\n",
    "\n",
    "### 🔧 성능 개선\n",
    "더 나은 결과를 얻기 위해 다음 기법들을 시도해보세요:\n",
    "\n",
    "1. **더 오래 학습**: `num_episodes`를 500-1000으로 증가\n",
    "2. **하이퍼파라미터 조정**: \n",
    "   - 낮은 학습률 (0.00005)\n",
    "   - 더 큰 버퍼 크기 (50000)\n",
    "   - 다른 epsilon 감소율 (0.999)\n",
    "3. **고급 기법**:\n",
    "   - Double DQN\n",
    "   - Dueling DQN\n",
    "   - Prioritized Experience Replay\n",
    "\n",
    "### 📚 더 알아보기\n",
    "- 원본 [DQN 논문](https://arxiv.org/abs/1312.5602) 읽기\n",
    "- Gymnasium의 다른 환경들 시도\n",
    "- DQN 변형 구현\n",
    "\n",
    "### 💾 작업 저장\n",
    "학습된 모델들이 `../models/saved_weights/`에 저장됩니다:\n",
    "- `dqn_best.pth` - 최고 성능 모델\n",
    "- `dqn_final.pth` - 학습 완료 후 최종 모델\n",
    "- `dqn_episode_X.pth` - 학습 중 체크포인트\n",
    "\n",
    "즐거운 레이싱 되세요! 🏎️💨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 요약\n",
    "print(\"🎉 DQN 학습 노트북 완료!\")\n",
    "print(\"=\"*50)\n",
    "print(\"✅ 환경 설정\")\n",
    "print(\"✅ DQN 네트워크 구조\")\n",
    "print(\"✅ 경험 재생 버퍼\")\n",
    "print(\"✅ 에이전트 학습\")\n",
    "print(\"✅ 결과 시각화\")\n",
    "print(\"\\n🚀 AI 에이전트가 레이싱 준비 완료!\")\n",
    "print(\"데모 스크립트를 실행하여 실제 동작을 확인하세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
