#!/usr/bin/env python3
"""
í•™ìŠµëœ DQN ì—ì´ì „íŠ¸ ì‹œì—°

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•™ìŠµëœ DQN ì—ì´ì „íŠ¸ê°€ CarRacingì„ í”Œë ˆì´í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê³  ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ë¬´ì‘ìœ„ í–‰ë™ê³¼ ë¹„êµí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python demo_trained_agent.py [--model model.pth] [--episodes 5] [--compare]

ì‘ì„±ì: DQN Racing Tutorial
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import gymnasium as gym
import cv2
import argparse
import os
import time
import matplotlib.pyplot as plt
from pathlib import Path
from collections import deque
from typing import Optional, Tuple, List, Dict
import pygame


# í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ import
import sys
sys.path.append(str(Path(__file__).parent.parent / "training"))
from dqn_training import DQN, CarRacingWrapper, HYPERPARAMETERS


# ============================================================================
# ì—ì´ì „íŠ¸ ë°ëª¨ í´ë˜ìŠ¤
# ============================================================================

class DQNDemo:
    """í•™ìŠµëœ DQN ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ ë°ëª¨ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: Optional[str] = None, render: bool = True):
        """
        ë°ëª¨ í™˜ê²½ ì´ˆê¸°í™”
        
        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ
            render: í™˜ê²½ ë Œë”ë§ ì—¬ë¶€
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ì‚¬ìš© ì¥ì¹˜: {self.device}")
        
        # í™˜ê²½ ì´ˆê¸°í™”
        render_mode = "human" if render else "rgb_array"
        self.env = CarRacingWrapper(render_mode=render_mode)
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.network = DQN(action_dim=4).to(self.device)
        
        # ëª¨ë¸ ë¡œë“œ (ì œê³µëœ ê²½ìš°)
        self.model_loaded = False
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            self._find_and_load_best_model()
            
        # ë°ëª¨ í†µê³„
        self.episode_rewards = []
        self.episode_lengths = []
        self.action_counts = []
        
    def load_model(self, model_path: str):
        """í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ ì²˜ë¦¬
            if 'main_network' in checkpoint:
                state_dict = checkpoint['main_network']
            elif 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
                
            self.network.load_state_dict(state_dict)
            self.network.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            
            print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            self.model_loaded = True
            
        except Exception as e:
            print(f"âœ— ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤")
            self.model_loaded = False
            
    def _find_and_load_best_model(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœê³ ì˜ ëª¨ë¸ì„ ì°¾ì•„ì„œ ë¡œë“œ"""
        models_dir = Path(__file__).parent.parent / "models" / "saved_weights"
        
        # ìµœê³  ëª¨ë¸ ì°¾ê¸°
        best_model = models_dir / "dqn_best.pth"
        if best_model.exists():
            self.load_model(str(best_model))
            return
            
        # ìµœì¢… ëª¨ë¸ ì°¾ê¸°
        final_model = models_dir / "dqn_final.pth"
        if final_model.exists():
            self.load_model(str(final_model))
            return
            
        # ì•„ë¬´ ëª¨ë¸ì´ë‚˜ ì°¾ê¸°
        if models_dir.exists():
            model_files = list(models_dir.glob("*.pth"))
            if model_files:
                # ìˆ˜ì • ì‹œê°„ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœê·¼ ê²ƒ ì„ íƒ
                latest_model = max(model_files, key=lambda x: x.stat().st_mtime)
                self.load_model(str(latest_model))
                return
                
        print("âš ï¸  í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ì„¸ìš”:")
        print("python training/dqn_training.py")
        
    def select_action(self, state: np.ndarray, use_model: bool = True) -> int:
        """
        í•™ìŠµëœ ëª¨ë¸ ë˜ëŠ” ë¬´ì‘ìœ„ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
            use_model: í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì„ íƒëœ í–‰ë™
            
        TODO: í•™ìŠµëœ DQN ëª¨ë¸ë¡œ í–‰ë™ì„ ì„ íƒí•˜ì„¸ìš”
        íŒíŠ¸ 1: use_modelì´ Trueì´ê³  ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆë‹¤ë©´, ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤
        íŒíŠ¸ 2: torch.no_grad()ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤
        íŒíŠ¸ 3: ìƒíƒœë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ê³  ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤ (unsqueeze(0))
        íŒíŠ¸ 4: ë„¤íŠ¸ì›Œí¬ë¡œ Q-ê°’ì„ ê³„ì‚°í•˜ê³  argmax()ë¡œ ìµœëŒ€ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤
        íŒíŠ¸ 5: use_modelì´ Falseì´ê±°ë‚˜ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¬´ì‘ìœ„ í–‰ë™ì„ ë°˜í™˜í•©ë‹ˆë‹¤
        """
        #YOUR CODE HERE
        raise NotImplementedError("í•™ìŠµëœ ëª¨ë¸ë¡œ í–‰ë™ ì„ íƒì„ êµ¬í˜„í•˜ì„¸ìš”")
            
    def run_episode(self, use_model: bool = True, max_steps: int = 1000) -> Tuple[float, int, List[int]]:
        """
        ì—ì´ì „íŠ¸ë¡œ ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰
        
        Args:
            use_model: í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
            max_steps: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜
            
        Returns:
            (ì´_ë³´ìƒ, ì—í”¼ì†Œë“œ_ê¸¸ì´, ìˆ˜í–‰í•œ_í–‰ë™ë“¤) íŠœí”Œ
        """
        state = self.env.reset()
        total_reward = 0.0
        actions_taken = []
        
        for step in range(max_steps):
            action = self.select_action(state, use_model)
            actions_taken.append(action)
            
            next_state, reward, terminated, truncated, _ = self.env.step(action)
            total_reward += reward
            state = next_state
            
            if terminated or truncated:
                break
                
        return total_reward, step + 1, actions_taken
        
    def demo_single_agent(self, num_episodes: int = 5, use_model: bool = True):
        """
        ì—¬ëŸ¬ ì—í”¼ì†Œë“œ ë™ì•ˆ ë‹¨ì¼ ì—ì´ì „íŠ¸ ë°ëª¨
        
        Args:
            num_episodes: ì‹¤í–‰í•  ì—í”¼ì†Œë“œ ìˆ˜
            use_model: í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
        """
        agent_type = "í•™ìŠµëœ DQN" if (use_model and self.model_loaded) else "ë¬´ì‘ìœ„"
        print(f"\n{'='*60}")
        print(f"{agent_type.upper()} ì—ì´ì „íŠ¸ ì‹œì—°")
        print(f"{'='*60}")
        
        episode_rewards = []
        episode_lengths = []
        all_actions = []
        
        for episode in range(num_episodes):
            print(f"\nì—í”¼ì†Œë“œ {episode + 1}/{num_episodes}")
            print("-" * 30)
            
            start_time = time.time()
            reward, length, actions = self.run_episode(use_model)
            episode_time = time.time() - start_time
            
            episode_rewards.append(reward)
            episode_lengths.append(length)
            all_actions.extend(actions)
            
            print(f"ë³´ìƒ: {reward:8.2f}")
            print(f"ê¸¸ì´: {length:4d} ìŠ¤í…")
            print(f"ì‹œê°„: {episode_time:6.2f}ì´ˆ")
            
            # ì´ ì—í”¼ì†Œë“œì˜ í–‰ë™ ë¶„í¬
            action_counts = np.bincount(actions, minlength=4)
            action_names = ['ì™¼ìª½', 'ì§ì§„', 'ì˜¤ë¥¸ìª½', 'ë¸Œë ˆì´í¬']
            print("í–‰ë™:", end="")
            for i, (name, count) in enumerate(zip(action_names, action_counts)):
                print(f" {name}: {count:3d}", end="")
            print()
            
        # ìš”ì•½ í†µê³„
        print(f"\n{agent_type} ì—ì´ì „íŠ¸ ìš”ì•½:")
        print("-" * 30)
        print(f"ì—í”¼ì†Œë“œ:      {num_episodes}")
        print(f"í‰ê·  ë³´ìƒ:     {np.mean(episode_rewards):8.2f}")
        print(f"í‘œì¤€í¸ì°¨:      {np.std(episode_rewards):8.2f}")
        print(f"ìµœê³  ë³´ìƒ:     {np.max(episode_rewards):8.2f}")
        print(f"ìµœì € ë³´ìƒ:     {np.min(episode_rewards):8.2f}")
        print(f"í‰ê·  ê¸¸ì´:     {np.mean(episode_lengths):6.1f} ìŠ¤í…")
        
        # ì „ì²´ í–‰ë™ ë¶„í¬
        total_action_counts = np.bincount(all_actions, minlength=4)
        print(f"í–‰ë™ ë¶„í¬:")
        for i, (name, count) in enumerate(zip(action_names, total_action_counts)):
            percentage = count / len(all_actions) * 100
            print(f"  {name:8}: {count:5d} ({percentage:5.1f}%)")
            
        return episode_rewards, episode_lengths
        
    def compare_agents(self, num_episodes: int = 5):
        """
        í•™ìŠµëœ ì—ì´ì „íŠ¸ vs ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ ë¹„êµ
        
        Args:
            num_episodes: ì—ì´ì „íŠ¸ë‹¹ ì—í”¼ì†Œë“œ ìˆ˜
        """
        if not self.model_loaded:
            print("âš ï¸  ë¹„êµí•  í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            print("ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ ë°ëª¨ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            self.demo_single_agent(num_episodes, use_model=False)
            return
            
        print(f"\n{'='*60}")
        print("ì—ì´ì „íŠ¸ ë¹„êµ")
        print(f"{'='*60}")
        
        # í•™ìŠµëœ ì—ì´ì „íŠ¸ ì‹¤í–‰
        print(f"\nğŸ¤– í•™ìŠµëœ DQN ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸...")
        trained_rewards, trained_lengths = self.demo_single_agent(num_episodes, use_model=True)
        
        # ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ ì‹¤í–‰
        print(f"\nğŸ² ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸...")
        random_rewards, random_lengths = self.demo_single_agent(num_episodes, use_model=False)
        
        # í†µê³„ì  ë¹„êµ
        print(f"\n{'='*60}")
        print("ë¹„êµ ê²°ê³¼")
        print(f"{'='*60}")
        
        print(f"{'ì§€í‘œ':<20} {'í•™ìŠµëœ DQN':<15} {'ë¬´ì‘ìœ„':<15} {'ê°œì„ ë„':<15}")
        print("-" * 65)
        
        # ë³´ìƒ
        trained_avg = np.mean(trained_rewards)
        random_avg = np.mean(random_rewards)
        reward_improvement = ((trained_avg - random_avg) / abs(random_avg)) * 100
        
        print(f"{'í‰ê·  ë³´ìƒ':<20} {trained_avg:<15.2f} {random_avg:<15.2f} {reward_improvement:<15.1f}%")
        
        # ì—í”¼ì†Œë“œ ê¸¸ì´
        trained_len_avg = np.mean(trained_lengths)
        random_len_avg = np.mean(random_lengths)
        length_improvement = ((trained_len_avg - random_len_avg) / random_len_avg) * 100
        
        print(f"{'í‰ê·  ê¸¸ì´':<20} {trained_len_avg:<15.1f} {random_len_avg:<15.1f} {length_improvement:<15.1f}%")
        
        # ìµœê³  ì„±ëŠ¥
        trained_best = np.max(trained_rewards)
        random_best = np.max(random_rewards)
        best_improvement = ((trained_best - random_best) / abs(random_best)) * 100
        
        print(f"{'ìµœê³  ë³´ìƒ':<20} {trained_best:<15.2f} {random_best:<15.2f} {best_improvement:<15.1f}%")
        
        # ì¼ê´€ì„± (ë‚®ì€ í‘œì¤€í¸ì°¨ê°€ ë” ì¢‹ìŒ)
        trained_std = np.std(trained_rewards)
        random_std = np.std(random_rewards)
        consistency_improvement = ((random_std - trained_std) / random_std) * 100
        
        print(f"{'ì¼ê´€ì„±':<20} {trained_std:<15.2f} {random_std:<15.2f} {consistency_improvement:<15.1f}%")
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        from scipy import stats
        try:
            t_stat, p_value = stats.ttest_ind(trained_rewards, random_rewards)
            print(f"\ní†µê³„ì  ê²€ì • (t-test):")
            print(f"  t-í†µê³„ëŸ‰: {t_stat:.3f}")
            print(f"  p-ê°’:     {p_value:.4f}")
            if p_value < 0.05:
                print("  ê²°ê³¼: í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤!")
            else:
                print("  ê²°ê³¼: ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (ë” ë§ì€ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤)")
        except ImportError:
            print("\ní†µê³„ì  ìœ ì˜ì„± ê²€ì •ì„ ìœ„í•´ scipyë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”")
            
        # ë¹„êµ ê·¸ë˜í”„ ìƒì„±
        self._plot_comparison(trained_rewards, random_rewards)
        
    def _plot_comparison(self, trained_rewards: List[float], random_rewards: List[float]):
        """ë¹„êµ ê·¸ë˜í”„ ìƒì„±"""
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # ì—í”¼ì†Œë“œ ë³´ìƒ ë¹„êµ
            episodes = range(1, len(trained_rewards) + 1)
            ax1.plot(episodes, trained_rewards, 'b-o', label='í•™ìŠµëœ DQN', linewidth=2)
            ax1.plot(episodes, random_rewards, 'r-s', label='ë¬´ì‘ìœ„', linewidth=2)
            ax1.set_xlabel('ì—í”¼ì†Œë“œ')
            ax1.set_ylabel('ë³´ìƒ')
            ax1.set_title('ì—í”¼ì†Œë“œ ë³´ìƒ ë¹„êµ')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Box plot ë¹„êµ
            ax2.boxplot([trained_rewards, random_rewards], 
                       labels=['í•™ìŠµëœ DQN', 'ë¬´ì‘ìœ„'])
            ax2.set_ylabel('ë³´ìƒ')
            ax2.set_title('ë³´ìƒ ë¶„í¬ ë¹„êµ')
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ê·¸ë˜í”„ ì €ì¥
            logs_dir = Path(__file__).parent.parent / "logs"
            logs_dir.mkdir(exist_ok=True)
            plot_path = logs_dir / "agent_comparison.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"\në¹„êµ ê·¸ë˜í”„ ì €ì¥: {plot_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")
            
    def interactive_demo(self):
        """ì‚¬ìš©ì ì¡°ì‘ì´ ê°€ëŠ¥í•œ ëŒ€í™”í˜• ë°ëª¨"""
        if not self.model_loaded:
            print("âš ï¸  í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return
            
        print(f"\n{'='*60}")
        print("ëŒ€í™”í˜• ë°ëª¨")
        print(f"{'='*60}")
        print("ì¡°ì‘ë²•:")
        print("  SPACE - í•™ìŠµëœ/ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ ì „í™˜")
        print("  R     - ì—í”¼ì†Œë“œ ë¦¬ì…‹")
        print("  ESC   - ì¢…ë£Œ")
        print("  P     - ì¼ì‹œì •ì§€/ì¬ê°œ")
        print("-" * 60)
        
        pygame.init()
        clock = pygame.time.Clock()
        
        use_model = True
        paused = False
        state = self.env.reset()
        episode_reward = 0.0
        episode_steps = 0
        
        running = True
        while running:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                elif event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        running = False
                    elif event.key == pygame.K_SPACE:
                        use_model = not use_model
                        agent_type = "í•™ìŠµëœ DQN" if use_model else "ë¬´ì‘ìœ„"
                        print(f"ì „í™˜ë¨: {agent_type}")
                    elif event.key == pygame.K_r:
                        state = self.env.reset()
                        episode_reward = 0.0
                        episode_steps = 0
                        print("ì—í”¼ì†Œë“œ ë¦¬ì…‹")
                    elif event.key == pygame.K_p:
                        paused = not paused
                        print("ì¼ì‹œì •ì§€" if paused else "ì¬ê°œ")
                        
            if not paused:
                action = self.select_action(state, use_model)
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                
                episode_reward += reward
                episode_steps += 1
                state = next_state
                
                # ì •ë³´ í‘œì‹œ
                agent_type = "DQN" if use_model else "ë¬´ì‘ìœ„"
                action_names = ['ì™¼ìª½', 'ì§ì§„', 'ì˜¤ë¥¸ìª½', 'ë¸Œë ˆì´í¬']
                print(f"\r{agent_type} | ìŠ¤í…: {episode_steps:4d} | "
                      f"ë³´ìƒ: {episode_reward:7.2f} | "
                      f"í–‰ë™: {action_names[action]}", end="", flush=True)
                
                if terminated or truncated:
                    print(f"\nì—í”¼ì†Œë“œ ì¢…ë£Œ! ìµœì¢… ë³´ìƒ: {episode_reward:.2f}")
                    state = self.env.reset()
                    episode_reward = 0.0
                    episode_steps = 0
                    
            clock.tick(30)  # 30 FPS
            
        pygame.quit()
        print("\nëŒ€í™”í˜• ë°ëª¨ ì¢…ë£Œ")
        
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.env.close()


# ============================================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================================

def main():
    """ì—ì´ì „íŠ¸ ë°ëª¨ë¥¼ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='DQN ì—ì´ì „íŠ¸ ë°ëª¨')
    parser.add_argument('--model', type=str, default=None,
                       help='í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--episodes', type=int, default=5,
                       help='ì‹¤í–‰í•  ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--compare', action='store_true',
                       help='í•™ìŠµëœ ì—ì´ì „íŠ¸ vs ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ ë¹„êµ')
    parser.add_argument('--interactive', action='store_true',
                       help='ëŒ€í™”í˜• ë°ëª¨ ì‹¤í–‰')
    parser.add_argument('--no-render', action='store_true',
                       help='ë Œë”ë§ ë¹„í™œì„±í™”')
    
    args = parser.parse_args()
    
    print("DQN ì—ì´ì „íŠ¸ ë°ëª¨")
    print("=" * 60)
    
    # ë°ëª¨ ìƒì„±
    demo = DQNDemo(model_path=args.model, render=not args.no_render)
    
    try:
        if args.interactive:
            demo.interactive_demo()
        elif args.compare:
            demo.compare_agents(args.episodes)
        else:
            demo.demo_single_agent(args.episodes, use_model=True)
            
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ë°ëª¨ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        print(f"ë°ëª¨ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    finally:
        demo.cleanup()
        
    print("\në°ëª¨ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
