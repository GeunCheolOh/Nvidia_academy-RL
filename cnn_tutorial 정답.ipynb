{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch CNN Tutorial\n",
        "## CIFAR-10 Image Classification\n",
        "\n",
        "CIFAR-10 데이터셋을 사용하여 이미지 분류 모델을 구축하고 학습시킵니다.\n",
        "\n",
        "### 목차\n",
        "1. Torch Tensor 기초\n",
        "2. Autograd (자동 미분)\n",
        "3. Torch nn 모듈\n",
        "4. 모델 만들기\n",
        "5. Optimizer와 Loss Function\n",
        "6. 데이터 준비\n",
        "7. 커스텀 데이터셋 클래스\n",
        "8. DataLoader\n",
        "9. Early Stopping\n",
        "10. 학습 및 평가 함수\n",
        "11. 모델 학습 실행\n",
        "12. 테스트\n",
        "13. Pretrained 모델 불러오기\n",
        "14. 전이학습\n",
        "15. 모델 저장 및 불러오기\n",
        "16. 추론 예시\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import timm\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 디바이스 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Torch Tensor 기초\n",
        "\n",
        "PyTorch의 기본 데이터 구조인 Tensor에 대해 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tensor 생성\n",
        "tensor_a = torch.tensor([1, 2, 3, 4, 5])\n",
        "tensor_b = torch.randn(3, 4)  # 3x4 랜덤 텐서\n",
        "tensor_c = torch.zeros(2, 3)  # 2x3 영행렬\n",
        "tensor_d = torch.ones(2, 3)   # 2x3 일행렬\n",
        "\n",
        "print(f\"tensor_a: {tensor_a}\")\n",
        "print(f\"tensor_b shape: {tensor_b.shape}\")\n",
        "print(f\"tensor_c: {tensor_c}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tensor 연산\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "y = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "# 기본 연산\n",
        "add_result = x + y\n",
        "mul_result = x * y  # element-wise\n",
        "matmul_result = torch.matmul(x, y)  # dot product\n",
        "\n",
        "print(f\"\\n덧셈: {add_result}\")\n",
        "print(f\"곱셈(element-wise): {mul_result}\")\n",
        "print(f\"내적: {matmul_result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 행렬곱\n",
        "matrix_a = torch.randn(2, 3)\n",
        "matrix_b = torch.randn(3, 4)\n",
        "matrix_mul = torch.matmul(matrix_a, matrix_b)\n",
        "print(f\"\\n행렬곱 결과 shape: {matrix_mul.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape 연산\n",
        "original = torch.randn(2, 3, 4)\n",
        "reshaped = original.view(2, 12)  # view: reshape과 유사\n",
        "print(f\"\\nOriginal shape: {original.shape}, Reshaped: {reshaped.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Autograd (자동 미분)\n",
        "\n",
        "PyTorch의 자동 미분 시스템에 대해 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# requires_grad=True로 gradient 추적\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "y = x ** 2 + 3 * x + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# y를 x로 미분\n",
        "y_sum = y.sum()\n",
        "y_sum.backward()\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")\n",
        "print(f\"dy/dx: {x.grad}\")  # 2*x + 3의 값\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient 초기화\n",
        "x.grad.zero_()\n",
        "print(f\"Gradient 초기화 후: {x.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 파이토치를 활용한 선형 회귀 연습\n",
        "선형 회귀(Linear Regression)는 머신러닝과 통계 분야에서 널리 사용되는 기본적인 예측 기법 중 하나입니다. 선형 회귀의 주된 목적은 데이터 포인트 간의 선형 관계를 파악하는 것입니다. 즉, 주어진 독립 변수(X)를 기반으로 종속 변수(Y)의 값을 예측하는 것입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 데이터 준비\n",
        "선형 회귀를 위한 학습 데이터를 준비합니다.\n",
        "\n",
        "우리가 사용할 데이터는 x와 y 사이에 간단한 선형 관계, y=2x 라는 관계를 가진 데이터를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(x_train)\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(y_train)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 가중치와 편향 초기화\n",
        "선형 회귀의 목표는 주어진 데이터에 대해 가장 잘 맞는 직선을 찾는 것입니다. 이 직선은 y = Wx + b로 표현될 수 있으며, 여기서\n",
        "\n",
        "\n",
        "**W는 가중치(weight)이고,**\n",
        "\n",
        "\n",
        "**b는 편향(bias)입니다.**\n",
        "\n",
        "\n",
        "데이터 학습을 시작하기 전에, 초기의\n",
        "W와 b 값을 정해줄 필요가 있습니다. 일반적으로는 랜덤 값으로 시작하나, 이 예제에서는 간단히\n",
        "W를 0으로 시작하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 업데이트 가능 / 학습가능\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b = torch.zeros(1, requires_grad=True)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 가설(hypothesis) 설정하기\n",
        "선형 회귀의 핵심은 주어진 x값에 대한 예측값 y를 찾는것 입니다. 이 예측값을 구하기 위해 가설 이라는 함수를 정의합니다 여기서는 간단한 선형 가설을 사용합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 순전파(Forward pass)\n",
        "hypothesis = x_train * W + b # y_pred(모델 예측값)\n",
        "print(hypothesis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 비용함수 및 최적화\n",
        "우리의 목표는 주어진 데이터에 가장 잘 맞는 직선을 찾는 것입니다. 이를 위해 실제값과 예측값 사이의 차이를 계산하는 비용 함수를 정의하게 됩니다. 선형 회귀에서는 주로 평균 제곱 오차(Mean Squared Error, MSE)를 비용 함수로 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**평균 제곱 오차(Mean Squared Error)**\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n}(\\text{예측값} - \\text{실제값})^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**최적화(optimization)**\n",
        "\n",
        "선형 회귀의 학습 과정은 이 비용을 최소화하는 가중치 W와 편향 b를 찾는 것입니다. 이를 위해 경사 하강법(Gradient Descent)와 같은 최적화 알고리즘이 사용됩니다. 파이토치에서는 다양한 최적화 알고리즘을 제공하며, 여기서는 SGD(Stochastic Gradient Descent)를 사용하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 손실 계산\n",
        "cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "print(cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 역전파\n",
        "\n",
        "# optimizer 선언 # 최적화 알고리즘 \n",
        "# Optimizer 역할 : 계산된 손실의 기울기(Gradient)를 기반으로 파라미터를 어떻게? 업데이트할지를 알려주는 로직\n",
        "optimizer = optim.SGD([W, b], lr=0.01) # lr = learning rate 변환의 범위\n",
        "# gradient를 0으로 초기화\n",
        "optimizer.zero_grad()\n",
        "# 비용 함수를 미분하여 gradient 계산\n",
        "cost.backward()\n",
        "# W와 b를 업데이트\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "W, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  또는 nn module로 선형 회귀\n",
        "# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n",
        "model = nn.Linear(1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(list(model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 모델학습\n",
        "학습 과정을 살펴 보겠습니다.\n",
        "\n",
        "1. 에포크(Epoch):\n",
        "전체 훈련 데이터가 학습에 한 번 사용된 주기를 말합니다. 여기서는 총 2000번의 에포크(0 ~ 1999) 동안 학습을 수행하도록 설정했습니다.\n",
        "\n",
        "2. 예측(Hypothesis/Prediction):  \n",
        "모델은 입력 x에 가중치 W를 곱하고 편향 b를 더하여 예측값을 계산합니다. 이 예측값은 hypothesis에 저장됩니다.\n",
        "\n",
        "3. 비용 함수(Cost Function):  \n",
        "예측값 hypothesis와 실제값 y_train 간의 오차를 평균 제곱 오차(MSE) 로 계산하여 cost에 저장합니다.\n",
        "\n",
        "4. 최적화(Gradient Descent):  \n",
        "계산된 cost를 바탕으로 경사 하강법을 통해 모델의 가중치 W와 편향 b를 업데이트합니다.\n",
        "\n",
        "5. 로깅(Logging):  \n",
        "학습 진행 상황을 모니터링하기 위해 100 에포크마다 W, b 및 cost 값을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_epochs = 2000 # EPOCH 학습 횟수\n",
        "\n",
        "# Epoch : 전체 데이터를 1회 완독\n",
        "# 모델이 업데이트 되는 주기는 1 Batch\n",
        "# 일반적으로 1 Epoch != 1 Batch\n",
        "# 우리는 한 Epoch 당 여러 번의 Batch로 분할하여 학습\n",
        "# e.g 데이터 1000개, Batch_size=100이라면 100개짜리 묶음을 10번 순회 > 10번 업데이트\n",
        "# 1 Batch 학습 > 1 Step\n",
        "\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    \n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(\n",
        "        prediction, y_train\n",
        "    )  # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
        "\n",
        "    # cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward()  # backward 연산\n",
        "    # W와 b를 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        # 100번마다 로그 출력\n",
        "        print(\"Epoch {:4d}/{} Cost: {:.6f}\".format(epoch, nb_epochs, cost.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 모델 추론론\n",
        "학습된 모델을 사용하여 새로운 데이터에 대한 예측을 수행해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 임의의 입력 4를 선언\n",
        "new_var = torch.FloatTensor([[4.0]])\n",
        "\n",
        "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var)  # forward 연산\n",
        "\n",
        "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
        "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 예제에서는 y = 2x 관계를 가지는 데이터로 모델을 학습시켰기 때문에, 입력값이 4일 때 예측값은 8에 가까운 값이 출력되어야 합니다. 이를 통해 모델이 정상적으로 학습되었음을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 모델 파라미터 확인\n",
        "PyTorch의 model.parameters() 메서드를 사용하면, 해당 모델의 모든 파라미터(가중치와 편향)를 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(list(model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Torch nn 모듈\n",
        "\n",
        "PyTorch의 신경망 모듈에 대해 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 레이어 선언\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "linear_layer = nn.Linear(in_features=128, out_features=10)\n",
        "relu = nn.ReLU()\n",
        "maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "print(f\"Conv2d layer: {conv_layer}\")\n",
        "print(f\"Linear layer: {linear_layer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 더미 텐서로 레이어 테스트\n",
        "dummy_image = torch.randn(1, 3, 32, 32)  # Batch=1, Channel=3, Height=32, Width=32\n",
        "conv_output = conv_layer(dummy_image)\n",
        "print(f\"\\n입력 shape: {dummy_image.shape}\")\n",
        "print(f\"Conv 출력 shape: {conv_output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 파라미터 확인\n",
        "print(f\"\\nConv layer의 파라미터:\")\n",
        "for name, param in conv_layer.named_parameters():\n",
        "    print(f\"  {name}: shape={param.shape}, requires_grad={param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# no_grad로 학습 방지\n",
        "print(f\"\\n학습 전 weight requires_grad: {conv_layer.weight.requires_grad}\")\n",
        "for param in conv_layer.parameters():\n",
        "    param.requires_grad = False\n",
        "print(f\"학습 방지 후 weight requires_grad: {conv_layer.weight.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 다시 학습 가능하게 설정\n",
        "for param in conv_layer.parameters():\n",
        "    param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 모델 만들기\n",
        "\n",
        "CNN 모델을 만드는 두 가지 방법을 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 방법 1: Sequential\n",
        "sequential_model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 8 * 8, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "print(sequential_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 방법 2: Subclassing (권장)\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNNModel, self).__init__()\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        \n",
        "        # Pooling\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Conv block 1\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
        "        \n",
        "        # Conv block 2\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
        "        \n",
        "        # Conv block 3\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # FC layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = CNNModel(num_classes=10)\n",
        "print(f\"\\nSubclassing Model:\\n{model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 파라미터 수 계산\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\n전체 파라미터: {total_params:,}\")\n",
        "print(f\"학습 가능한 파라미터: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optimizer와 Loss Function\n",
        "\n",
        "학습에 필요한 최적화 알고리즘과 손실 함수에 대해 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer 설정\n",
        "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer_adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "print(f\"SGD optimizer: {optimizer_sgd}\")\n",
        "print(f\"Adam optimizer: {optimizer_adam}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion_ce = nn.CrossEntropyLoss()\n",
        "criterion_mse = nn.MSELoss()\n",
        "\n",
        "print(f\"\\nCrossEntropyLoss: {criterion_ce}\")\n",
        "\n",
        "# Optimizer 사용 예시\n",
        "optimizer = optimizer_adam  # Adam 사용\n",
        "criterion = criterion_ce\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 데이터 준비 - CIFAR-10\n",
        "\n",
        "CIFAR-10 데이터셋을 다운로드하고 전처리합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform 정의\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CIFAR-10 다운로드\n",
        "full_train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', \n",
        "    train=True,\n",
        "    download=True, \n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', \n",
        "    train=False,\n",
        "    download=True, \n",
        "    transform=transform_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 분할: Train/Validation\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation dataset에 test transform 적용\n",
        "val_dataset.dataset.transform = transform_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 클래스 이름\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 커스텀 데이터셋 클래스 예시\n",
        "\n",
        "커스텀 데이터셋을 만드는 방법을 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomCIFAR10(Dataset):\n",
        "    \"\"\"\n",
        "    커스텀 데이터셋 클래스 예시\n",
        "    실제로는 위의 torchvision.datasets.CIFAR10을 사용하지만,\n",
        "    커스텀 데이터셋을 만드는 방법을 보여주기 위한 예시입니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.targets[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. DataLoader\n",
        "\n",
        "데이터를 배치 단위로 로드하는 DataLoader를 설정합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=batch_size,\n",
        "    shuffle=True, \n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, \n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, \n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "# 샘플 배치 확인\n",
        "sample_batch = next(iter(train_loader))\n",
        "sample_images, sample_labels = sample_batch\n",
        "print(f\"\\nSample batch - Images shape: {sample_images.shape}\")\n",
        "print(f\"Sample batch - Labels shape: {sample_labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 학습 및 평가 함수\n",
        "\n",
        "모델 학습과 평가를 위한 함수들을 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"한 에포크 학습\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # 통계\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        # 프로그레스 바 업데이트\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{running_loss / (pbar.n + 1):.4f}',\n",
        "            'acc': f'{100. * correct / total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_epoch(model, loader, criterion, device):\n",
        "    \"\"\"한 에포크 검증\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc='Validation')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{running_loss / (pbar.n + 1):.4f}',\n",
        "                'acc': f'{100. * correct / total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
        "                num_epochs, device, best_model_path='best_model.pth'):\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    best_model_wts = None\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 50)\n",
        "        \n",
        "        # 학습\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        \n",
        "        # 검증\n",
        "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "        \n",
        "        # 히스토리 저장\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        # best val_loss 모델 저장\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = model.state_dict()\n",
        "            torch.save(best_model_wts, best_model_path)\n",
        "            print(f'>> Best model saved! val_loss: {val_loss:.4f}')\n",
        "        \n",
        "        # 결과 출력\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        \n",
        "    return history\n",
        "\n",
        "print(\"전체 학습 루프 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 모델 학습 실행\n",
        "\n",
        "CNN 모델을 학습시킵니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 초기화\n",
        "model = CNNModel(num_classes=10).to(device)\n",
        "\n",
        "# Optimizer와 Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# 학습 실행\n",
        "num_epochs = 20\n",
        "history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=num_epochs,\n",
        "    device=device,\n",
        "    best_model_path='best_model.pth'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 곡선 시각화\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_loss'], label='Train Loss')\n",
        "axes[0].plot(history['val_loss'], label='Val Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history['train_acc'], label='Train Acc')\n",
        "axes[1].plot(history['val_acc'], label='Val Acc')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Training history saved to 'training_history.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. 테스트\n",
        "\n",
        "학습된 모델을 테스트 데이터로 평가합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# best model을 불러오는 코드\n",
        "best_model = torch.load('best_model.pth', map_location=device)\n",
        "model.load_state_dict(best_model)\n",
        "print(\"Best model loaded from 'best_model.pth'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, test_loader, device):\n",
        "    \"\"\"테스트 데이터로 모델 평가\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # 클래스별 정확도 계산을 위한 변수\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            \n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            # 클래스별 정확도\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(len(labels)):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "    \n",
        "    # 전체 정확도\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'\\nTest Accuracy: {accuracy:.2f}%')\n",
        "    \n",
        "    # 클래스별 정확도\n",
        "    print('\\nClass-wise Accuracy:')\n",
        "    for i in range(10):\n",
        "        class_acc = 100 * class_correct[i] / class_total[i]\n",
        "        print(f'{classes[i]:>10s}: {class_acc:.2f}%')\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "test_accuracy = test_model(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Pretrained 모델 불러오기\n",
        "\n",
        "사전 훈련된 모델을 불러오는 방법을 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torchvision에서 모델 불러오기\n",
        "print(\"[torchvision models]\")\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "print(f\"ResNet18 loaded: {type(resnet18)}\")\n",
        "\n",
        "# 마지막 레이어 수정 (CIFAR-10은 10개 클래스)\n",
        "num_features = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(num_features, 10)\n",
        "print(f\"Modified final layer for 10 classes\")\n",
        "\n",
        "# timm에서 모델 불러오기\n",
        "print(\"\\n[timm models]\")\n",
        "efficientnet = timm.create_model('efficientnet_b0', pretrained=True, num_classes=10)\n",
        "print(f\"EfficientNet-B0 loaded: {type(efficientnet)}\")\n",
        "\n",
        "# timm에서 사용 가능한 모델 확인 (처음 10개만)\n",
        "available_models = timm.list_models(pretrained=True)[:10]\n",
        "print(f\"\\nAvailable pretrained models (first 10): {available_models}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. 전이학습 (Transfer Learning)\n",
        "\n",
        "사전 훈련된 모델을 활용한 전이학습 방법을 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ResNet18로 전이학습 예시\n",
        "model_transfer = models.resnet18(pretrained=True)\n",
        "\n",
        "# 방법 1: 모든 레이어 동결 (Feature Extractor로 사용)\n",
        "print(\"[방법 1] Feature Extractor - 모든 레이어 동결\")\n",
        "for param in model_transfer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 마지막 레이어만 학습 가능하게 설정\n",
        "num_features = model_transfer.fc.in_features\n",
        "model_transfer.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "# 학습 가능한 파라미터만 optimizer에 전달\n",
        "optimizer_transfer = optim.Adam(model_transfer.fc.parameters(), lr=0.001)\n",
        "\n",
        "trainable_params_1 = sum(p.numel() for p in model_transfer.parameters() if p.requires_grad)\n",
        "print(f\"학습 가능한 파라미터 수: {trainable_params_1:,}\")\n",
        "\n",
        "# 방법 2: 일부 레이어만 학습 (Fine-tuning)\n",
        "print(\"\\n[방법 2] Fine-tuning - 마지막 몇 개 레이어만 학습\")\n",
        "model_transfer2 = models.resnet18(pretrained=True)\n",
        "\n",
        "# 처음 레이어들은 동결\n",
        "for name, param in model_transfer2.named_parameters():\n",
        "    if 'layer4' not in name and 'fc' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# 마지막 레이어 수정\n",
        "model_transfer2.fc = nn.Linear(model_transfer2.fc.in_features, 10)\n",
        "\n",
        "# 학습 가능한 파라미터만 optimizer에 전달\n",
        "optimizer_transfer2 = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model_transfer2.parameters()), \n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "trainable_params_2 = sum(p.numel() for p in model_transfer2.parameters() if p.requires_grad)\n",
        "print(f\"학습 가능한 파라미터 수: {trainable_params_2:,}\")\n",
        "\n",
        "# 방법 3: 전체 모델 Fine-tuning (작은 learning rate 사용)\n",
        "print(\"\\n[방법 3] Full Fine-tuning - 전체 모델 학습 (작은 LR)\")\n",
        "model_transfer3 = models.resnet18(pretrained=True)\n",
        "model_transfer3.fc = nn.Linear(model_transfer3.fc.in_features, 10)\n",
        "\n",
        "# Differential learning rate: Backbone은 작은 LR, 새 레이어는 큰 LR\n",
        "optimizer_transfer3 = optim.Adam([\n",
        "    {'params': model_transfer3.layer4.parameters(), 'lr': 1e-4},\n",
        "    {'params': model_transfer3.fc.parameters(), 'lr': 1e-3}\n",
        "])\n",
        "\n",
        "print(f\"Backbone LR: 1e-4, New layer LR: 1e-3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 전이학습 모델로 간단히 학습 (3 에포크만)\n",
        "print(\"\\n전이학습 모델 학습 시작 (3 epochs)...\")\n",
        "model_transfer = model_transfer.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(3):\n",
        "    print(f'\\nEpoch {epoch+1}/3')\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model_transfer, train_loader, criterion, optimizer_transfer, device\n",
        "    )\n",
        "    val_loss, val_acc = validate_epoch(\n",
        "        model_transfer, val_loader, criterion, device\n",
        "    )\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. 모델 저장 및 불러오기\n",
        "\n",
        "학습된 모델을 저장하고 불러오는 방법을 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 저장\n",
        "torch.save(model.state_dict(), 'cifar10_cnn_model.pth')\n",
        "print(\"모델 가중치 저장 완료: cifar10_cnn_model.pth\")\n",
        "\n",
        "# 전체 모델 저장 (권장하지 않음, 하지만 가능함)\n",
        "torch.save(model, 'cifar10_cnn_full_model.pth')\n",
        "print(\"전체 모델 저장 완료: cifar10_cnn_full_model.pth\")\n",
        "\n",
        "# 모델 불러오기\n",
        "loaded_model = CNNModel(num_classes=10)\n",
        "loaded_model.load_state_dict(torch.load('cifar10_cnn_model.pth'))\n",
        "loaded_model = loaded_model.to(device)\n",
        "loaded_model.eval()\n",
        "print(\"\\n모델 가중치 불러오기 완료\")\n",
        "\n",
        "# 체크포인트 저장 (optimizer 상태 포함)\n",
        "checkpoint = {\n",
        "    'epoch': num_epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'history': history\n",
        "}\n",
        "torch.save(checkpoint, 'checkpoint.pth')\n",
        "print(\"체크포인트 저장 완료: checkpoint.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. 추론 예시\n",
        "\n",
        "학습된 모델을 사용하여 새로운 이미지에 대한 예측을 수행합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트 이미지 하나 가져오기\n",
        "sample_image, sample_label = test_dataset[0]\n",
        "sample_image_batch = sample_image.unsqueeze(0).to(device)  # 배치 차원 추가\n",
        "\n",
        "# 추론\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(sample_image_batch)\n",
        "    probabilities = F.softmax(output, dim=1)\n",
        "    predicted_class = output.argmax(dim=1).item()\n",
        "    confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "print(f\"실제 클래스: {classes[sample_label]}\")\n",
        "print(f\"예측 클래스: {classes[predicted_class]}\")\n",
        "print(f\"신뢰도: {confidence * 100:.2f}%\")\n",
        "\n",
        "# Top-5 예측\n",
        "top5_prob, top5_classes = torch.topk(probabilities, 5)\n",
        "print(\"\\nTop-5 예측:\")\n",
        "for i in range(5):\n",
        "    print(f\"{i+1}. {classes[top5_classes[0][i]]}: {top5_prob[0][i].item() * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "kaggle_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
